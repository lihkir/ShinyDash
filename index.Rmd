--- 
title: "Visualización de Datos con R Sección 3"
author: "Lihki Rubio"
university: "Universidad del Norte"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
geometry: margin=2cm
bibliography: [book.bib, packages.bib]
link-citations: yes
github-repo: rstudio/bookdown-demo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = "center", 
                      warning = FALSE, 
                      message = FALSE)
```


# Modelo de series de tiempo

En esta sección estudiaremos la forma de implementar el modelo `ARIMA` estudiado antes en la sección de `Python`. La teoría abordada para este modelo es la misma, por lo tanto procedemos a la implementación en `R` de este modelo predictivo

## Modelo `ARIMA` 

- El modelo de **Media Móvil Integrada Autorregresiva** `(ARIMA)` es el nombre genérico de una familia de modelos predictivos que se basan en los procesos **Autorregresivo** `(AR)` y de **Media Móvil** `(MA)`.

- Entre los modelos predictivos tradicionales (por ejemplo, la **regresión lineal, suavización exponencial**, etc.), el modelo `ARIMA` se considera el enfoque más avanzado y robusto. 

- En esta sección, presentaremos los componentes del modelo: los procesos `AR` y `MA` y el componente de diferenciación. Además, nos centraremos en los métodos y enfoques para ajustar los parámetros del modelo con el uso de **diferenciación**, la función de **autocorrelación** `(ACF)` y la función de **autocorrelación parcial** `(PACF)`.

- En esta sección cubriremos lo siguiente

  - El estado estacionario de los datos de las series temporales
  - El proceso **random walk**
  - Los procesos `AR` y `MA`
  - Los modelos `ARMA` y `ARIMA`
  - El modelo ARIMA estacional

- Librerías:
  
  - `forecast`
  - `TSstudio`
  - `plotly`
  - `dplyr`
  - `lubridate`
  - `stats`
  - `datasets`
  - `base`

## El proceso estacionario

- Uno de los principales supuestos de la familia de modelos `ARIMA` es que las series de entrada siguen la estructura de un proceso estacionario. Este supuesto se basa en el **Teorema de representación de Wold**, que afirma que cualquier proceso estacionario puede representarse como una combinación lineal de ruido blanco. 

- Por lo tanto, antes de sumergirnos en los componentes del modelo `ARIMA`, vamos a detenernos a hablar del **proceso estacionario**. El **proceso estacionario**, en el contexto de los datos de las series temporales, describe un estado estocástico de la serie. Los datos de las series temporales son estacionarios si se dan las siguientes condiciones: 
  - La media y la varianza de la serie no cambian con el tiempo 
  - La estructura de correlación de la serie, junto con sus rezagos, permanece igual a lo largo del tiempo tiempo

- En los siguientes ejemplos, utilizaremos la función `arima.sim` del paquete `stats` para simular unos datos de series temporales estacionarios y no estacionarios y los representaremos con la función `ts_plot` del paquete `TSstudio`. La función `arima.sim` nos permite simular datos de series temporales basándonos en los componentes y características principales del modelo `ARIMA`:
  - Un `proceso autorregresivo (AR)`: Establece una relación entre la serie y sus $p$ rezagos pasados con el uso de un modelo de regresión
  - Un `proceso de media móvil (MA)`: Similar al proceso `AR`, el proceso `MA` establece la relación con el término de error en el tiempo $t$ y los términos de error pasados, con el uso de regresión entre los dos componentes
- `Proceso integrado (I)`: El proceso de diferenciar la serie con sus $d$ rezagos para transformar la serie en un estado estacionario. Aquí, el argumento del modelo de la función define $p, q$ y $d$, así como el orden de los procesos `AR, MA`, y los procesos `I` del modelo. 

- Consideraríamos una serie temporal de datos como no estacionaria siempre que las condiciones mencionadas anteriormente no se cumplan. Los ejemplos más comunes de una serie con una estructura no estacionaria son los siguientes:

  - Una serie con una tendencia dominante: La media de la serie cambia a lo largo del tiempo como función del cambio en la tendencia de la serie, y por tanto la serie es no estacionaria
  - Una serie con un componente estacional multiplicativo: En este caso, la varianza de la serie es una función de la oscilación estacional a lo largo del tiempo, que aumenta o disminuye con el tiempo

- La serie clásica `AirPassenger` (el número mensual de pasajeros de las aerolíneas entre 1949 y 1960) del paquete de conjuntos de datos es un buen ejemplo de una serie que viola las dos condiciones del proceso estacionario. Dado que la serie tiene tanto una fuerte tendencia lineal como un estacional multiplicativo, tanto la media como la varianza cambian con el tiempo:


```{r}
library(TSstudio)

data(AirPassengers)
ts_plot(AirPassengers,
        title = "Monthly Airline Passenger Numbers 1949-1960",
        Ytitle = "Thousands of Passengers",
        Xtitle = "Year")
```

## Transformación de una serie no estacionaria en una serie serie estacionaria

- En la mayoría de los casos, a menos que tenga mucha suerte, sus datos brutos probablemente vendrán con una tendencia u otra forma de oscilación que viole los supuestos del proceso estacionario. Por lo tanto, para manejar esto, tendrá que aplicar algunos pasos de transformación con el fin de llevar la serie a un estado estacionario. Los métodos de transformación más comunes son la diferenciación la serie (o la des-tendencia) y la transformación logarítmica (o ambas). Repasemos las aplicaciones de estos métodos.

## Diferenciación de series temporales

- El método más habitual para transformar los datos de una serie temporal no estacionaria en un estado estacionario es diferenciar la serie con sus rezagos. El principal efecto de diferenciar una serie es la eliminación de la tendencia de la serie (o la pérdida de tendencia de la serie), que ayuda a estabilizar la media de la serie. Medimos el grado u orden de diferenciación de la serie por el número de veces que diferenciamos la serie con sus rezagos. Por ejemplo, la siguiente ecuación define la diferencia de primer orden:

$$
Y_{t}'=Y_{t}-Y_{t-1}
$$

- Aquí $Y_{t}'$, representa la diferencia de primer orden de la serie, y $Y_{t}, Y_{t-1}$ representan la serie y su primer rezago respectivamente. En algunos casos, el uso de la diferencia de primer orden no es suficiente para llevar la serie a un estado estacionario, y es posible que desee aplicar la diferencia de segundo orden:

$$
Y_{t}^{*}=Y_{t}^{'}-Y_{t-1}^{'}=(Y_{t}-Y_{t-1})-(Y_{t-1}-Y_{t-2})=
Y_{t}-2Y_{t-1}+Y_{t-2}
$$
- Otra forma de diferenciación es la diferenciación estacional, que se basa en diferenciar la series con el desfase estacional:

$$
Y_{t}^{'}=Y_{t}-Y_{t-f}
$$

- Aquí, $f$ representa la frecuencia de la serie y $Y_{t-f}$ representa el rezago estacional de la serie. La función `diff` del paquete base diferencia la serie de entrada con un rezago específico, configurando el argumento `lag` de rezago correspondiente. Volvamos a la serie serie `AirPassenger` y veamos cómo el primer orden y la diferenciación estacional afectan a la estructura de la serie. Empezaremos con la diferencia de primer orden:

```{r}
library(TSstudio)

data(AirPassengers)

ts_plot(diff(AirPassengers, lag = 1),
        title = "AirPassengers Series - First Differencing",
        Xtitle = "Year",
        Ytitle = "Differencing of Thousands of Passengers")

```

- Puede ver que la primera diferencia de la serie `AirPassenger` eliminó la tendencia de la serie y que la media de la serie es, en general, constante en el tiempo. Por otra parte, hay una clara evidencia de que la variación de la serie está aumentando con el tiempo, y por lo tanto la serie aún no es estacionaria. Además de la diferencia de primer orden, tomar la diferencia estacional de la serie podría resolver este problema. Añadamos la diferencia estacional a la diferencia de primer orden y trazémosla de nuevo:

```{r}
library(TSstudio)

data(AirPassengers)

ts_plot(diff(diff(AirPassengers, lag = 1), 12),
        title = "AirPassengers Series - First and Seasonal Differencing",
        Xtitle = "Year",
        Ytitle = "Differencing of Thousands of Passengers")
```

- La diferencia estacional ha conseguido estabilizar la variación de la serie, ya que ahora la serie parece ser estacionaria.

## Transformación logarítmica

Podemos utilizar **transformación logarítmica** para estabilizar una oscilación estacional multiplicativa, si es que existe. Este enfoque no es un sustitución de la diferenciación, sino una adición. Por ejemplo, en el caso de `AirPassenger` de la sección anterior, vimos que la primera diferenciación hace un gran trabajo al estabilizar la media de la serie, pero no es suficiente para estabilizar la varianza de la serie. Por lo tanto, podemos aplicar una transformación logarítmica para transformar la estructura estacional de multiplicable a aditiva y luego aplicar la diferencia de primer orden para estacionar la serie:

```{r}
ts_plot(diff(log(AirPassengers), lag = 1),
        title = "AirPassengers Series - First Differencing with Log Transformation",
        Xtitle = "Year",
        Ytitle = "Differencing/Log of Thousands of Passengers")
```

- La transformación logarítmica con la diferenciación de primer orden está haciendo un mejor trabajo de transformación de la serie en un estado estacionario con respecto a la doble diferenciación (de primer orden con
diferenciación estacional) que utilizamos anteriormente.


## Proceso `ARIMA`

Una de las limitaciones de los modelos `AR, MA` y `ARMA` es que no pueden manejar datos de series temporales no estacionarias. Por lo tanto, si la serie de entrada es no estacionaria, se requiere un paso de preprocesamiento para transformar la serie de un estado no estacionario a un estado estacionario. El modelo `ARIMA` ofrece una solución para este problema al añadir el proceso integrado para el modelo `ARMA`. El proceso integrado `(I)` consiste simplemente en diferenciar la serie con sus rezagos, donde el grado de diferenciación está representado por el parámetro `d`. Podemos generalizar el proceso de diferenciación con la siguiente ecuación:

$$
Y_{d}=(Y_{t}-Y_{t-1})-\cdots-(Y_{t-d+1}-Y_{t-d}),
$$
donde $Y_{d}$ es la $d$ diferenciación de la series. Añadamos el componente de diferenciación al modelo ARMA y formalicemos el modelo ARIMA:

$$
\textsf{ARIMA}(p, d, q): Y_{d}=c+\sum_{i=1}^{p}\phi_{i}Y_{d-i}+\sum_{i=1}^{q}\theta_{i}\epsilon_{t-i}+\epsilon_{t},
$$
donde

  - `ARIMA`$(p,d,q)$ define un proceso `ARIMA` con un proceso `AR` de orden $p$, un grado $d$ de diferenciación, y un proceso `MA` de orden $q$
  - $Y_{d}$ es la diferencia $d$ de la serie $Y_{t}$
  - $c$ representa una constante (o desviación)
  - $p$ define el número de rezagos que se regresan contra $Y_{t}$
  - $Y_{d-i}$ es el coeficiente del rzago $i$ de la serie
  - $q$ define el número de términos de error pasados que se utilizarán en la ecuación
  - $\theta_{i}$ es el coeficiente correspondiente de $\epsilon_{t-i}$
  - $\epsilon_{t-q},\dots,\epsilon_{t}$ son términos de error de ruido blanco
  - $\epsilon_{t}$ representa el término de error, que es ruido blanco

Se pueden representar los modelos `AR, MA` o `ARMA` con el modelo `ARIMA`, por ejemplo:

- El modelo `ARIMA(0, 0, 0)` es equivalente al ruido blanco
- El modelo `ARIMA(0, 1, 0)` es equivalente a un paseo aleatorio
- El modelo `ARIMA(1, 0, 0)` es equivalente a un proceso AR(1)
- El modelo `ARIMA(0, 0, 1)` es equivalente a un proceso MA(1)
- El modelo `ARIMA(1, 0, 1)` es equivalente a un proceso ARMA(1,1)

## Identificación del grado de diferenciación del modelo

- Al igual que los parámetros $p$ y $q$, el ajuste del parámetro $d$ (el grado de diferenciación de la de la serie) puede hacerse con los gráficos `ACF` y `PACF`. En el siguiente ejemplo, utilizaremos los precios mensuales del café Robusta desde el año 2000. Esta serie forma parte del objeto `Coffee_Prices` del paquete `TSstudio`. Empezaremos cargando la serie `Coffee_Prices` y restando los precios mensuales del café Robusta desde enero de 2010 utilizando la función `window`:

```{r}
data("Coffee_Prices")
robusta_price <- window(Coffee_Prices[,1], start = c(2000, 1))

ts_plot(robusta_price,
        title = "The Robusta Coffee Monthly Prices",
        Ytitle = "Price in USD",
        Xtitle = "Year")
```

- Como se puede ver, los precios del café Robusta a lo largo del tiempo tienen una tendencia al alza, por lo que no se encuentra en un estado estacionario. Además, como esta serie representa precios continuos, es probable que la serie tenga una fuerte relación de correlación con sus rezagos pasados (ya que los cambios en el precio son típicamente cercanos al precio anterior). Volveremos a utilizar la función `acf` para identificar el tipo de relación entre la serie y sus rezagos:

```{r}
acf(robusta_price)
```

- Como se puede ver en la salida anterior del gráfico `ACF`, la correlación de la serie con sus rezagos decae lentamente en el tiempo de forma lineal. La eliminación de la tendencia de la serie y de la correlación entre la serie y sus rezagos puede hacerse por diferenciación. Comenzaremos Comenzaremos con la primera diferenciación utilizando la función `diff`:

```{r}
robusta_price_d1 <- diff(robusta_price)
par(mfrow=c(1,2))
acf(robusta_price_d1)
pacf(robusta_price_d1)
```

- Los gráficos `ACF` y `PACF` de la primera diferencia de la serie indican que un proceso `AR(1) `es apropiado para utilizar en la serie diferenciada, ya que el `ACF` es de cola y el `PACF` corta en el primer rezago. Por lo tanto, aplicaremos un modelo `ARIMA(1,1,0)` a la serie `robusta_price` para incluir la primera diferencia:

```{r}
library(forecast)

robusta_md <- arima(robusta_price, order = c(1, 1, 0))
```

- Utilizaremos la función `summary` para revisar los detalles del modelo:

```{r}
summary(robusta_md)
```

- Puede ver en el resumen del modelo que el coeficiente `ar1` es estadísticamente significativo. Por último, pero no menos importante, comprobaremos los residuos del modelo:

```{r}
checkresiduals(robusta_md)
```

- En general, el gráfico de los residuos del modelo y la prueba de `Ljung-Box` indican que los residuos son `white noise`. El gráfico `ACF` indica que hay algunos rezagos correlacionados, pero ellos son sólo significativos ewn los bordes, por lo que podemos ignorarlos.


- Muchas pruebas estadísticas se utilizan para intentar rechazar alguna hipótesis nula. En este caso concreto, la prueba de `Ljung-Box` trata de rechazar la independencia de algunos valores

  - Si el valor $p < 0,05$: Puedes rechazar la hipótesis nula asumiendo un 5% de probabilidad de error. Por lo tanto, puede asumir que sus valores muestran dependencia entre sí.

  - Si el valor $p > 0,05$: No tiene suficiente evidencia estadística para rechazar la hipótesis nula. Así que no puede asumir que sus valores son dependientes. Esto podría significar que sus valores son dependientes de todos modos o puede significar que sus valores son independientes. Pero usted no está probando ninguna posibilidad específica, lo que su prueba realmente es que usted no puede afirmar la dependencia de los valores, ni puede afirmar la independencia de los valores.

  - En general, lo importante aquí es tener en cuenta que un valor $p < 0.05$ permite rechazar la hipótesis nula, pero un valor $p > 0.05$ no permite confirmar la hipótesis nula. Esto es, no se puede probar la independencia de los valores de las series temporales utilizando la prueba de `Ljung-Box`. Sólo puede probar la dependencia.

## Predicción del consumo mensual de gas natural en EE.UU

- El modelo **ARIMA estacional (SARIMA)**, como su nombre indica, es una versión designada del modelo `ARIMA` para series temporales con un componente estacional. Una serie temporal con un componente estacional tiene una fuerte relación con sus rezagos estacionales, el modelo `SARIMA` utiliza los rezagos estacionales de manera similar a como el modelo ARIMA utiliza los rezagos no estacionales con los procesos `AR` y `MA` y la diferenciación. Para ello, añade los tres componentes siguientes al modelo `ARIMA`

  - **Proceso SAR(P)**: Un proceso `A`R estacional de la serie con sus `P` rezagos estacionales pasados. Por ejemplo, un `SAR(2)` es un proceso `AR` de la serie con sus dos últimos rezagos estacionales, es decir, $Y_{t}=c+\Phi_{1}Y_{t-f}+\Phi_{2}Y_{t-2f}+\epsilon_{t}$ donde $\Phi$ representa el coeficiente estacional del proceso `SAR`, y $f$ representa la frecuencia de la serie.
  
  - **Proceso SMA(Q)**: Un proceso `MA` estacional de la serie con sus `Q` términos de error estacionales pasados. Por ejemplo, un `SMA(1)` es un proceso de media móvil de la serie con su término de error estacional pasado, es decir, $Y_{t}=\mu+\epsilon_{t}+\Theta_{1}\epsilon_{t-f}$, donde $\Theta$ representa el coeficiente estacional del proceso `SMA`, y $f$ representa la frecuencia de la serie.
  - **Proceso SI(D)**: Una diferenciación estacional de la serie con sus últimos `D` rezagos estacionales. De forma similar, podemos diferenciar la serie con su rezago estacional, es decir $Y_{D=1}=Y_{t}-Y_{t-f}$.

- Utilizamos la siguiente notación para denotar los parámetros `SARIMA`, donde los parámetros $P$ y $Q$ representan el orden correspondiente de los procesos `AR` y `MA` estacionales de la serie con sus rezagos estacionales, y `D` define el grado de diferenciación de la serie con sus rezagos no estacionales.

$$
\textsf{SARIMA}(p, d, q)\times(P, D, Q),
$$

- Aplicaremos lo aprendido acerca del modelo `ARIMA` para pronosticar el consumo mensual de gas natural en Estados Unidos. Vamos a cargar la serie `USgas` del paquete `TSstudio`:

```{r}
library(TSstudio)
data(USgas)

ts_plot(USgas,
        title = "US Monthly Natural Gas consumption",
        Ytitle = "Billion Cubic Feet",
        Xtitle = "Year")
```

- Como hemos visto anteriormente, la serie de gas de EE.UU. tiene un fuerte patrón estacional, además la serie tiene una tendencia al alza, por lo tanto, se requiere el uso de los modelos `ARIMA`. Empezaremos por definir particiones `training` y `testing` usando la función `ts_split`, dejando los últimos 12 meses de la serie como partición de prueba:

```{r}
USgas_split <- ts_split(USgas, sample.out = 12)

train <- USgas_split$train
test <- USgas_split$test

```

- - Antes de iniciar el proceso de entrenamiento del modelo `SARIMA`, realizaremos diagnósticos con respecto a la correlación de la serie con las funciones `ACF` y `PACF`. Dado que nos interesa ver la relación de la serie con sus rezagos estacionales, aumentaremos el número de rezagos a calcular y mostrar, estableciendo el argumento `lag.max` a 60 rezagos

```{r}
par(mfrow=c(1,2))
acf(train, lag.max = 60)
pacf(train, lag.max = 60)
```

- El gráfico `ACF` anterior indica que la serie tiene una fuerte correlación tanto con los estacional como con los rezagos no estacionales. Además, el decaimiento lineal de los rezagos estacionales indica que la serie no es estacionaria y que se requiere una diferenciación estacional. Comenzaremos con diferenciación estacional de la serie y trazaremos el resultado para identificar si la serie se encuentra en un estado estacionario:

```{r}
USgas_d12 <- diff(train, 12)

ts_plot(USgas_d12,
        title = "US Monthly Natural Gas consumption - First Seasonal Difference",
        Ytitle = "Billion Cubic Feet (First Difference)",
        Xtitle = "Year")
```

- Aunque hemos eliminado la tendencia de la serie, la variación de la serie aún no es estable. Por lo tanto, también intentaremos tomar la primera diferencia de la serie:

```{r}
USgas_d12_1 <- diff(diff(USgas_d12, 1))

ts_plot(USgas_d12_1,
        title = "US Monthly Natural Gas consumption - First Seasonal and Non-Seasonal Differencing",
        Ytitle = "Billion Cubic Feet (Difference)",
        Xtitle = "Year")
```

- Después de tomar la diferenciación de primer orden, junto con la diferenciación estacional de primer orden, la serie parece estabilizarse en torno a la línea del eje $x$ cero (o bastante cerca de ser estable). Después de transformar la serie en un estado estacionario, podemos revisar las funciones `ACF` y `PACF` de nuevo para identificar el proceso necesario

```{r}
par(mfrow=c(1,2))
acf(USgas_d12_1, lag.max = 60)
pacf(USgas_d12_1, lag.max = 60)
```

- La principal observación de los gráficos `ACF` y `PACF` anteriores es que tanto los rezagos no estacionales como los rezagos estacionales (en ambos gráficos) se están reduciendo. Por lo tanto, podemos concluir que después de de diferenciar las series y transformarlas en un estado estacionario, deberíamos aplicar un proceso proceso `ARMA` para los componentes estacionales y no estacionales del modelo `SARIMA`

## La función `auto.arima`

- Uno de los principales retos de la previsión con la familia de modelos `ARIMA` es el proceso de ajuste de los modelos. Como hemos visto en este capítulo, este proceso incluye muchos pasos manuales que se requieren para verificar la estructura de las series (estacionarias o no estacionaria), las transformaciones de los datos, el análisis descriptivo con los gráficos ACF y PACF para identificar el tipo de proceso y, finalmente, afinar los parámetros del modelo. 

- Si bien entrenar un modelo ARIMA para una sola serie puede llevar unos minutos, es posible que no se pueda hacer si se tienen docenas de series que pronosticar. La función `auto.arima` del paquete `forecast` ofrece una solución a este problema. Este algoritmo automatiza el proceso de ajuste del modelo `ARIMA` con el uso de métodos para identificar tanto la estructura de la serie (estacionaria o no) como el tipo (estacional o no), y establece los parámetros del modelo en consecuencia. Por ejemplo, podemos utilizar la función para pronosticar `USgas`:


```{r}
library(forecast)

USgas_auto_md <- auto.arima(train)
USgas_auto_md
```

- Utilizando los argumentos por defecto de la función `auto.arima` se obtiene el modelo `ARIMA` que minimice la puntuación `AIC`. En este caso, se seleccionó un modelo con una puntuación `AIC` de 2480,57:

- Por defecto, la función `auto.arima` aplica una búsqueda de modelos más corta utilizando un enfoque por pasos para reducir el tiempo de búsqueda. La contrapartida de este enfoque es que el modelo puede pasar por alto algunos modelos que pueden obtener mejores resultados. 

- Podemos improvisar con los resultados de `auto.arima` ajustando el argumento de búsqueda del modelo. El argumento `step-wise`, cuando se establece en `FALSE`, permite establecer una búsqueda más robusta y exhaustiva, con el coste de un mayor tiempo de búsqueda. Esta compensación entre el rendimiento y el tiempo de cálculo puede equilibrarse siempre que se tenga un conocimiento previo de la estructura y las características de la serie. Por ejemplo, volvamos a entrenar el conjunto de entrenamiento de la serie `USgas` esta vez con la siguiente configuración:

  - Establezca los parámetros de diferenciación `d` y `D` en 1.
  - Limite el orden del modelo a siete utilizando el argumento `max.order`. El argumento `max.order` define los valores máximos de $p+q+P+Q$, por lo que deberíamos fijarlo en cinco (dado que $d$ y $D$ están fijados en 1).
  -  Bajo estas restricciones, busque todas las combinaciones posibles estableciendo el argumento argumento `stepwise` a `FALSE`.
  - Establezca el argumento de `aproximation` en `FALSE` para obtener cálculos más precisos de los criterios de información:

```{r}
USgas_auto_md2 <- auto.arima(train,
                             max.order = 5,
                             D = 1,
                             d = 1,
                             stepwise = FALSE,
                             approximation = FALSE)
USgas_auto_md2
```

- Utilicemos el modelo entrenado `USgas_best_md2` para pronosticar las observaciones correspondientes del del conjunto de `testing`:

```{r}
USgas_test_fc <- forecast(USgas_auto_md2, h = 12)
```

- Evaluamos el rendimiento del modelo con la función de precisión:

```{r}
accuracy(USgas_test_fc, test)
```

- Ahora, utilizaremos la función `test_forecast` para obtener una visión más intuitiva del rendimiento del modelo del modelo en las particiones de entrenamiento y de prueba:


```{r}
test_forecast(USgas,
              forecast.obj = USgas_test_fc,
              test = test)
```

- Ahora que hemos cumplido las condiciones anteriores, podemos pasar al último paso del proceso de predicción y generar la predicción final con el modelo seleccionado. Empezaremos por reentrenando el modelo seleccionado en toda la serie:

```{r}
final_md <- arima(USgas, order = c(1,1,1), 
                  seasonal = list(order = c(2,1,1)))
```

- Antes de pronosticar los próximos 12 meses, verifiquemos que los residuos del modelo satisfacen la condición del modelo:

```{r}
checkresiduals(final_md)
```

- Observando el gráfico de residuos anterior, se puede ver que los residuos son `white noise` y se distribuyen normalmente. Además, la prueba de `Ljung-Box` confirma que no hay autocorrelación con un valor $p$ de 0.12, por lo tanto no podemos rechazar la hipótesis nula de que los residuos son `white noise`. Estamos listos para empezar. Utilicemos la función de predicción para
pronosticar los próximos 12 meses de la serie `USgas`:

```{r}
USgas_fc <- forecast(final_md, h = 12)
```

- Podemos trazar la predicción con la función `plot_forecast`:

```{r}
plot_forecast(USgas_fc,
              title = "US Natural Gas Consumption - Forecast",
              Ytitle = "Billion Cubic Feet",
              Xtitle = "Year")
```


# Análisis con datos faltantes

## Introducción

- Los datos faltantes es uno de los temas que se ignoran en la mayoría de los textos introductorios. Probablemente, parte de la razón por la que esto es así es que todavía abundan muchos mitos sobre el análisis con datos ausentes. Además, algunas de las investigación sobre técnicas de vanguardia es aún relativamente nueva. Una razón más legítima para su ausencia en los textos introductorios es que la mayoría de las metodologías más de principios son bastante complicadas, desde el punto de vista matemático. Sin embargo, la increíble ubicuidad de los problemas relacionados con los datos faltantes en el análisis de datos de la vida real requiere que se aborde el tema. Esta sección sirve como una suave introducción al tema y a una de las técnicas más eficaces para tratarla. Un refrán común sobre el tema es algo así como que la mejor manera de tratar con los datos que faltan es no tenerlos. Es cierto que los datos que faltan son un tema complicado, y hay muchas maneras de hacerlo mal. Es importante no llevar este consejo al extremo, sin embargo, para eludir los problemas de los datos perdidos, algunos han impedido participantes en una encuesta, que por ejemplo, siguen sin responder a todas las preguntas de un formulario. 

- Hay tratamientos para los datos que faltan, pero no hay tratamientos para los datos malos. El tratamiento estándar para el problema de los datos que faltan es reemplazar los datos que faltan por valores no ausentes. Este proceso se denomina imputación. En la mayoría de los casos, el objetivo de la imputación no es recrear el conjunto de datos completo perdido, sino permitir que se realicen estimaciones o inferencias estadísticas válidas a partir de los datos perdidos. Por ello, la eficacia de las diferentes técnicas de imputación no puede evaluarse por su capacidad de recrear los datos con la mayor exactitud posible a partir de un conjunto de datos perdidos simulado, sino que deben juzgarse por su capacidad de apoyar las mismas inferencias estadísticas que se obtendrían del análisis de los datos completos que se extraen del análisis. 

- De este modo, rellenar los datos que faltan es sólo un paso hacia el verdadero objetivo: el análisis. El conjunto de datos imputados rara vez se considera el objetivo final de la imputación. En la práctica, hay muchas formas diferentes de tratar los datos que faltan, algunas son buenas y otras no tanto. Algunas están bien en determinadas circunstancias, pero no en otras. Algunas implican la eliminación de datos perdidos, mientras que otras implican la imputación. Vamos a mencionar brevemente algunos de los métodos más comunes. Sin embargo, el objetivo final de, es iniciarle en lo que a menudo se describe como el estándar de oro de las técnicas de imputación: la imputación múltiple.

## Visualización de los datos que faltan

- Para demostrar la visualización de patrones de datos ausentes, primero tenemos que crear algunos datos ausentes. Este será también el mismo conjunto de datos sobre el que realizaremos el análisis más adelante en el capítulo. Para mostrar cómo utilizar la imputación múltiple en un escenario semirealista, vamos a crear una versión del conjunto de datos `mtcars` con algunos valores perdidos: Configuremos `seed`  (para la aleatoriedad determinista), y creemos una variable para mantener nuestro nuevo conjunto de datos. Los datos se extrajeron de la revista **Motor Trend US** de 1974 y comprenden el consumo de combustible y 10 aspectos del diseño y el rendimiento de 32 automóviles (modelos de 1973 a 1974).

```{r}
set.seed(2)
miss_mtcars <- mtcars
```

- En primer lugar, vamos a crear siete valores faltantes en `drat` (alrededor del 20 por ciento), cinco valores faltantes en la columna `mpg` (alrededor del 15 por ciento), cinco valores faltantes en la columna `cyl`, tres valores faltantes en `wt` (alrededor del 10 por ciento), y tres valores faltantes en `vs`

```{r}
some_rows <- sample(1:nrow(miss_mtcars), 7)
miss_mtcars$drat[some_rows] <- NA

some_rows <- sample(1:nrow(miss_mtcars), 5)
miss_mtcars$mpg[some_rows] <- NA

some_rows <- sample(1:nrow(miss_mtcars), 5)
miss_mtcars$cyl[some_rows] <- NA

some_rows <- sample(1:nrow(miss_mtcars), 3)
miss_mtcars$wt[some_rows] <- NA

some_rows <- sample(1:nrow(miss_mtcars), 3)
miss_mtcars$vs[some_rows] <- NA
```

- Ahora, vamos a crear cuatro valores que faltan en `qsec`, pero sólo para los coches automáticos

```{r}
only_automatic <- which(miss_mtcars$am==0)
some_rows <- sample(only_automatic, 4)
miss_mtcars$qsec[some_rows] <- NA
```

Ahora, echemos un vistazo al conjunto de datos:

```{r}
head(miss_mtcars)
```

- Ahora vamos a visualizar los datos faltantes. La primera forma en que vamos a visualizar el patrón de los datos faltantes es utilizando la función función `md.pattern` del paquete `mice` (que es también el paquete que vamos a utilizar para imputar nuestros datos faltantes). Si no tiene el paquete
instálelo antes

```{r}
library(mice)
md.pattern(miss_mtcars)
```

- Un patrón de datos faltantes por fila se refiere a las columnas que faltan en cada fila. Esta función agrega y cuenta el número de filas con el mismo patrón de datos perdidos. Esta función produce una matriz binaria (0 y 1). Las celdas con un 1 representan datos no faltantes; los 0s representan datos que faltan. Como las filas están ordenadas en un orden creciente de ausencia, la primera fila siempre se refiere al patrón de datos ausentes que contiene la menor cantidad de datos faltantes.

- La columna más a la izquierda es un conteo del número de filas que muestran el patrón de datos faltantes, y la columna más a la derecha es un recuento del número de puntos de datos perdidos en ese patrón. La última fila contiene un recuento del número de puntos de datos que faltan en cada columna. Como puede ver, 12 de las filas no contienen datos perdidos. 

- Sólo hay seis filas que contienen más de un valor perdido. Sólo una de estas
filas contiene más de dos valores perdidos (como se muestra en la penúltima fila). En cuanto a los conjuntos de datos con datos faltantes, este en particular no contiene mucho. No es raro que en algunos conjuntos de datos falte más del 30% de los datos. Este conjunto de datos no llega ni al 3%. Ahora vamos a visualizar el patrón de datos faltantes gráficamente utilizando el paquete `VIM`. Probablemente también tenga que instalarlo.

```{r}
library(VIM)
aggr(miss_mtcars, numbers=TRUE)
```

```{r}
marginplot(miss_mtcars[c(1,2)])
```

- A simple vista, esta representación nos muestra, sin esfuerzo, que la columna `drat` representa la mayor proporción de faltas, por columnas, seguida de `mpg, cyl, qsec, vs.` y `wt`. El gráfico de la derecha nos muestra información similar a la de la salida de `md.pattern`. Esta representación, sin embargo, hace que sea más fácil saber si hay algún patrón sistemático de omisión. Las celdas azules representan los datos no ausentes, y las rojas representan los datos que faltan. Los números de la derecha del gráfico representan la proporción de filas que muestran ese patrón de datos perdidos. El 37,5% de las filas no contienen ningún tipo de dato que falte.

- El diagrama de caja rojo de la izquierda muestra la distribución de `cyl` sin `mpg`, mientras que el diagrama de caja azul muestra la distribución de los restantes puntos de datos. Lo mismo ocurre con los gráficos de caja de `mpg` en la parte inferior del gráfico. Si los datos tienen mecanismo `MCAR` se espera que los gráficos de caja rojos y azules sean muy similares.

## Tipos de datos faltantes

El paquete `VIM` nos permitió visualizar los patrones de datos que faltaban. Un término relacionado, el mecanismo de datos perdidos, describe el proceso que determina la probabilidad de que cada punto de datos sea faltante. Hay tres categorías principales de mecanismos de datos perdidos: **Missing Completely At Random (MCAR), Missing At Random (MAR)**, y **Missing Not At Random (MNAR)**. La discriminación basada en el mecanismo de datos perdidos es crucial, ya que nos informa sobre las opciones para manejar los datos faltantes.

- **Faltantes completamente al azar `(MCAR)`**

  - El primer mecanismo, `MCAR`, se produce cuando la falta de datos no está relacionada con la datos. Esto ocurriría, por ejemplo, si se borraran filas de una base de datos al azar, o si una ráfaga de viento se llevara una muestra aleatoria de los formularios de encuesta de un investigador. El mecanismo que rige la ausencia de `drat, mpg, cyl, wt`, y `vs`' es `MCAR`, porque seleccionamos al azar los elementos que faltan. Este mecanismo, aunque es el más fácil de trabajar, rara vez es sostenible en la práctica.

- **Faltantes no aleatorios (MNAR)**

  - El mecanismo `MNAR` se produce cuando la ausencia de una variable está relacionada con la variable en sí misma. Por ejemplo, supongamos que la báscula que pesa cada coche tiene una capacidad de sólo 3.700 libras y, por ello, los ocho coches que pesaban más se registraron como `NA`. Este es un ejemplo clásico del mecanismo `MNAR`, es el propio peso de la observación la causa de que falte. Otro ejemplo sería si durante el transcurso del ensayo de un fármaco antidepresivo los participantes a los que no les ayudaba el fármaco se deprimían demasiado para continuar con el ensayo. Al final del ensayo, cuando se accede y se registra el nivel de depresión de todos los participantes, habrá valores que falten para los participantes cuyo motivo de ausencia está relacionado con su nivel de depresión.


- **Faltantes al azar `(MAR)`**

  - El mecanismo, `faltante al azar`, tiene un nombre un tanto desafortunado, al contrario de lo que pueda parecer, significa que existe una relación sistemática entre la ausencia de una variable de resultado y otras variables observadas, pero no la variable de resultado en sí misma.
  
  - La mejor manera de explicarlo es con el siguiente ejemplo: Supongamos que en una encuesta hay una pregunta sobre el nivel de ingresos. Debido a ello, un gran número de participantes en la encuesta cuya lengua materna no es el inglés no pudo interpretar la pregunta y la dejaron en blanco. Si la encuesta sólo recogía el nombre, el sexo y los ingresos, el mecanismo de datos faltantes de la pregunta sobre los ingresos sería `MNAR`. Sin embargo, si el cuestionario incluía una pregunta sobre si el participante hablaba inglés como como primera lengua, el mecanismo sería `MAR`. La inclusión de la variable *Is English* significa que la falta de respuesta a la pregunta sobre los ingresos puede puede explicarse por completo. La razón por la que el nombre falta al azar es que cuando se controla la relación entre la variable omitida y la(s) variable(s) observada(s) con la que está relacionada (por ejemplo, ¿Cuál es su renta? y ¿Es el inglés su primera lengua? respectivamente), los datos faltan al azar.

  - Como otro ejemplo, existe una relación sistemática entre las variables `am` y `qsec` en nuestro conjunto de datos simulados que faltan: los `qsecs` sólo faltan en los coches automáticos. Pero dentro del grupo de coches automáticos, la variable `qsec` falta al azar. Por lo tanto, el mecanismo de `qsec` es `MAR`; controlando el tipo de transmisión, `qsec` falta al azar. Tenga en cuenta, sin embargo, que si eliminamos `am` de nuestro conjunto de datos simulado, `qsec` se convertiría en MNAR.
  

- **Observación**
  
  - Quizá haya observado que el lugar que ocupa un conjunto de datos concreto en la taxonomía del mecanismo de datos perdidos depende de las variables que incluye. Por ejemplo, sabemos que el mecanismo detrás de `qsec` es `MAR`, pero si el conjunto de datos no incluyera `am`, sería `MNAR`. Como somos nosotros los que creamos los datos, sabemos el procedimiento que dio lugar a los valores perdidos de `qsec`. Si no fuéramos nosotros los que creamos los datos, como ocurre en el mundo real, y el conjunto de datos no contuviera la columna `am` simplemente veríamos una cantidad de valores `qsec` que faltan arbitrariamente. Esto podría llevarnos a creer que los datos son `MCAR`. Sin embargo, no lo es; sólo porque la variable con la cual otra variable faltante está sistemáticamente relacionada no se observa, no quiere decir que esta no exista. Esto plantea una cuestión crítica: ¿podemos estar seguros de que nuestros datos no son `MCAR`? La respuesta desafortunada es no. Dado que los datos que necesitamos para demostrar o refutar el `MNAR` faltan *ipso facto*, la suposición de `MNAR` nunca puede ser desconfirmada de forma concluyente. Es nuestro trabajo, como analistas de datos con pensamiento crítico, preguntar si es probable que haya un mecanismo `MNAR` o no.


### Visualización de datos faltantes

- Tomando el conjunto de datos `airquality`, un conjunto de datos de mediciones diarias de la calidad del aire en Nueva York de mayo a septiembre de 1973, que tiene valores `NA` dentro de sus variables. Las filas del conjunto de datos representan 154 días consecutivos. Cualquier eliminación de estas filas afectará a la continuidad del tiempo, lo que puede *afectar a cualquier análisis de series temporales que se realice*. Veamos con más detalle el conjunto de datos de la calidad del aire

- Iniciamos visualizando los datos faltantes. Eliminaremos algunos puntos de datos del conjunto de datos para este ejemplo. En lo que respecta a las variables categóricas, la sustitución de las mismas no suele ser aconsejable. Algunas prácticas comunes incluyen la sustitución de las variables categóricas que faltan por la moda de las observadas, sin embargo, es cuestionable si es una buena opción. Aunque en este caso no faltan puntos de datos de las variables categóricas, las eliminamos de nuestro conjunto de datos (podemos volver a añadirlas más tarde si es necesario) y echamos un vistazo a los datos utilizando `summary()`

```{r}
data <- airquality
data[4:10, 3] <- rep(NA, 7)
data[1:5, 4] <- NA
```

```{r}
data <- data[-c(5,6)]
summary(data)
```

- Al parecer, el `Ozone` es la variable con más puntos de datos perdidos. A continuación vamos a profundizar en los patrones de datos que faltan. Suponiendo que los datos sean `MCAR`, demasiados datos perdidos también pueden ser un problema. Por lo general, un umbral máximo seguro es el 5% del total para conjuntos de datos grandes. Si los datos que faltan para una determinada característica o muestra son superiores al 5%, probablemente deba dejar de lado esa característica o muestra. Por lo tanto, comprobamos las características (columnas) y las muestras (filas) en las que falta más del 5% de los datos mediante una sencilla función


```{r}
pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(data,2,pMiss)
apply(data,1,pMiss)
```

- Vemos que al ozono le falta casi el 25% de los puntos de datos, por lo que podríamos considerar la posibilidad de eliminarlo del análisis o de reunir más mediciones. Las demás variables están por debajo del umbral del 5%, por lo que podemos mantenerlas. En lo que respecta a las muestras, la falta de una sola característica supone un 25% de datos perdidos por muestra. Las muestras a las que les faltan 2 o más características (> 50%) deben ser eliminadas si es posible.

- Usemos `md.pattern()` para conocer mejor el patrón de los datos que faltan


```{r}
library(mice)
md.pattern(data)
```

- La salida nos dice que 104 muestras están completas, que a 34 muestras les falta sólo la medición de `Ozone`, que a 4 muestras les falta sólo el valor de `Solar.R` y así sucesivamente. Una representación visual quizás más útil puede obtenerse utilizando el paquete `VIM` de la siguiente manera

```{r}
library(VIM)

aggr_plot <- aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data),
                  cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

- El gráfico nos ayuda a entender que a casi el 70% de las muestras no les falta ninguna información, al 22% le falta el valor de `Ozono` y las restantes muestran otros patrones de ausencia. A través de este enfoque, la situación parece un poco más clara. Otra aproximación visual (esperemos) útil es un gráfico de caja especial


```{r}
marginplot(data[c(1,2)])
```

- Obviamente, aquí estamos limitados a trazar sólo 2 variables a la vez, pero sin embargo podemos obtener algunas ideas interesantes. El diagrama de caja rojo de la izquierda muestra la distribución de `Solar.R` sin `Ozono`, mientras que el diagrama de caja azul muestra la distribución de los restantes puntos de datos. Lo mismo ocurre con los gráficos de caja de `Ozono` en la parte inferior del gráfico. Si nuestra suposición de los datos `MCAR` es correcta, entonces esperamos que los gráficos de caja rojos y azules sean muy similares.

### Borrado de la lista

- El método más utilizado por los científicos de datos para tratar los datos que faltan es simplemente omitir los casos con datos que faltan, analizando únicamente el resto del conjunto de datos. Este método se conoce como eliminación por lista o análisis de casos completos. La función `na.omit()` en `R` elimina todos los casos con uno o más valores de datos perdidos en un conjunto de datos.

- La mayor ventaja de este método es su comodidad. Sin embargo, si la naturaleza de los datos es `MCAR`, la eliminación de la lista dará lugar a errores estándar que son significativos para los datos reducidos, pero no para todo el conjunto de datos, que tenía los datos que faltaban. Este método de tratar los datos que faltan es posiblemente un desperdicio. Si se eliminan los casos de esta manera, hay que ser consciente de la disminución de la capacidad para detectar el verdadero efecto de las variables de interés.

- Sin embargo, si los datos no son `MCAR`, el análisis de casos completos puede influir mucho en las estimaciones de la media, los coeficientes de regresión y las correlaciones. La supresión de la lista puede causar submuestras sin sentido.


- A continuación, realizamos una simple función `na.omit()` para eliminar los casos que tienen `NAs`. Vemos que todas las filas que contenían algún `NA` en cualquier variable son eliminadas del dataframe

```{r}
airquality_omit <- na.omit(data)
head(airquality_omit)
```

- Dibujemos los histogramas antes y después de la imputación usando `ggplot`

```{r}
library(ggplot2)
library(tidyverse)
library(hrbrthemes)
library(gridExtra)
```


```{r}
ggp1 <- ggplot(data.frame(value=data$Ozone), aes(x=value)) +
  geom_histogram(fill="#FBD000", color="#E52521", alpha=0.9) +
  ggtitle("Original data") +
  xlab('Ozone') + ylab('Frequency') +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))

ggp2 <- ggplot(data.frame(value=airquality_omit$Ozone), aes(x=value)) +
  geom_histogram(fill="#43B047", color="#049CD8", alpha=0.9) +
  ggtitle("Listwise Deletion") +
  xlab('Ozone') + ylab('Frequency') +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))

grid.arrange(ggp1, ggp2, ncol = 2)
```

### Imputación de la media

- Algunos científicos de datos o estadísticos pueden buscar una solución rápida sustituyendo los datos que faltan por la media. La media se utiliza a menudo para imputar datos categóricos. Por ejemplo, en el conjunto de datos de calidad del aire, supongamos que queremos imputar la media de sus valores perdidos. En este caso, utilizamos el paquete `R` `mice`. Cambiando el argumento `method = mean`, se especifica la imputación de la media, el argumento `m = 1` cambia las iteraciones a 1, lo que significa no (iteración).

- El fundamento teórico de utilizar la media para imputar los datos perdidos es que la media es una buena estimación para seleccionar aleatoriamente una observación de una distribución normal. Ahora, intentaremos imputar la media para las variables `Ozono` y `Solar.R` del conjunto de datos `airquality`. En primer lugar, vamos a cargar los paquetes `mice` y `mipackages` (instale el paquete de `CRAN` primero usando `install.packages("mice")` y `install.packages("mi")`). Para mas información sobre el paquete utlizado visitar https://cran.r-project.org/web/packages/mice/mice.pdf. Podemos recuperar el conjunto de datos completo utilizando la función `complete()`.

```{r}
library(mice)
library(foreign)
```

- La media de las variables `Ozono` y `Solar.R` puede ser imputada por la función `mice()`. 

```{r}
imp <- mice(data, m=5, maxit=50, method ='pmm', seed=500, printFlag = FALSE)
```

- Donde `m=5` se refiere al número de conjuntos de datos imputados. Cinco es el valor por defecto. `meth='pmm'` se refiere al método de imputación. En este caso, utilizamos el método de imputación de coincidencia de medias predictivas. Se pueden utilizar otros métodos de imputación, escriba `methods(mice)` para obtener una lista de los métodos de imputación disponibles.

```{r}
imp_df <- complete(imp)
head(imp_df)
```

- Veamos un histograma y un diagrama de dispersión del aspecto del conjunto de datos de la calidad del aire tras la imputación de la media


```{r}
ggp1 <- ggplot(data.frame(value=data$Ozone), aes(x=value)) +
  geom_histogram(fill="#FBD000", color="#E52521", alpha=0.9) +
  ggtitle("Original data") +
  xlab('Ozone') + ylab('Frequency') +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))

ggp2 <- ggplot(data.frame(value=imp_df$Ozone), aes(x=value)) +
  geom_histogram(fill="#43B047", color="#049CD8", alpha=0.9) +
  ggtitle("Mean imputation") +
  xlab('Ozone') + ylab('Frequency') +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))

grid.arrange(ggp1, ggp2, ncol = 2)
```

- Vamos a comparar las distribuciones de los datos originales e imputados utilizando algunos gráficos útiles. En primer lugar, podemos utilizar un gráfico de dispersión y comparar `Ozone` con todas las demás variables

```{r}
library(lattice)
xyplot(imp, Ozone ~ Wind + Temp + Solar.R, pch=18, cex=1)
```

- Lo que queremos ver es que la forma de los puntos (imputados) coincida con la de los azules (observados). La coincidencia de la forma nos indica que los valores imputados son realmente *valores plausibles*. Otro gráfico útil es el de la densidad:

```{r}
densityplot(imp)
```

- La densidad de los datos imputados para cada conjunto de datos imputados se muestra en color magenta, mientras que la densidad de los datos observados se muestra en azul. De nuevo, según nuestros supuestos anteriores, esperamos que las distribuciones sean similares.

- Es muy recomendable comprobar visualmente la convergencia. Lo comprobamos cuando llamamos a la función de trazado en la variable a la que asignamos la salida de `mice`, para mostrar gráficos de seguimiento de la media y desviación estándar de todas las variables implicadas en las imputaciones. 

```{r}
plot(imp)
```

- Cada línea es una de las `m` imputaciones. Como se puede ver en el gráfico de trazado anterior sobre `imp`, no hay tendencias claras y las variables se superponen de una iteración a la siguiente. Dicho de otro modo, la varianza dentro de una cadena (hay `m` cadenas) debería ser aproximadamente igual a la varianza entre las cadenas. Esto indica que se ha logrado la convergencia. Si no se logra la convergencia, se puede aumentar el número de iteraciones que `mice` emplea especificando explícitamente el parámetro `maxit` a la función `mice`. Los siguientes son ejemplos de no convergencia usando el alogritmo `mice`


![](figures/imputation1.png)
![](figures/imputation2.png)

### Otras técnicas de imputación

- La función `mice` entrega distintos métodos de imputación los cuales puede estudiar y aplicar, de acuerdo a lo que requieran sus datos. Puede aplicar la `imputación por regresión` en R con la configuración del método `method = "norm.predict"` en la función `mice`. Puede aplicar la imputación de `regresión estocástica` en `R` con la función `mice` utilizando el método `method = "norm.nob"`. El paquete `mice` también incluye un procedimiento de imputación de regresión estocástica bayesiana. Puede aplicar este procedimiento de imputación con la función `mice` y utilizar como método `method = "norm"`. Para estos métodos debe seleccionar primero dos columna del conjunto de datos de interes para ajustar constantes del modelo.

```{r}
data <- data[, c("Solar.R", "Wind")]
imp.regress <- mice(data, method="norm.predict", m=1, maxit=1)
imp.regress$imp$Wind
```

- En la actualidad, hay un número limitado de análisis que pueden ser automáticamente agrupados por  `mice` el más importante es el de `lm/glm`. Sin embargo, si se recuerda, el modelo lineal generalizado es extremadamente flexible, y puede utilizarse para expresar una amplia gama de análisis diferentes. Por extensión, podríamos utilizar la imputación múltiple no sólo para regresión lineal, sino para la regresión logística, la regresión de Poisson, las pruebas $t$, el `ANOVA`
`ANCOVA`, etc. Cada una de estas temáticas pueden ser abordadas como proyectos en este curso.

## Detección de valores atípicos

- Un valor atípico es un valor o una observación que se aleja de otras observaciones, es decir, un punto de datos que difiere significativamente de otros puntos de datos. *Enderlein (1987)* va incluso más allá, ya que el autor considera que los valores atípicos son aquellos que se desvían tanto de otras observaciones que se podría suponer un mecanismo de muestreo subyacente diferente.

- Una observación debe compararse siempre con otras observaciones realizadas sobre el mismo fenómeno antes de calificarla realmente de atípica. En efecto, una persona de 200 cm de altura (1,90 m en EE.UU.) se considerará probablemente un valor atípico en comparación con la población general, pero esa misma persona podría no considerarse un valor atípico si midiéramos la altura de los jugadores de baloncesto.

- En esta sección, presentamos varios enfoques para detectar valores atípicos en `R`, desde técnicas sencillas como la estadística descriptiva (que incluye el mínimo, el máximo, el histograma, el boxplot y los percentiles) hasta técnicas más formales como el filtro de `Hampel`, el `Grubbs`, el `Dixon` y las pruebas de `Rosner` para detectar valores atípicos. Algunas pruebas estadísticas exigen la ausencia de valores atípicos para sacar conclusiones sólidas, pero la eliminación de valores atípicos no se recomienda en todos los casos y debe hacerse con precaución.

### Estadísticas descriptivas

- El primer paso para detectar los valores atípicos en `R` es comenzar con algunas estadísticas descriptivas, y en particular con el mínimo y el máximo. En `R`, esto se puede hacer fácilmente con la función `summary()`

```{r}
dat <- ggplot2::mpg
summary(dat$hwy)
```

- Nótese que el mínimo y el máximo son, respectivamente, el primer y el último valor de la salida anterior. Como alternativa, también pueden calcularse con las funciones `min()` y `max()`

```{r}
min(dat$hwy)
```

```{r}
max(dat$hwy)
```

- Un claro error de codificación, como un peso de 786 kg (1733 libras) para un humano, ya se detectará fácilmente con esta técnica tan sencilla.

### Histogramas 

- Otra forma básica de detectar valores atípicos es dibujar un histograma de los datos. Utilizando la base de `R` (con el número de `bins` correspondiente a la raíz cuadrada del número de observaciones para tener más `bins` que la opción por defecto), o usando `ggplot2`

```{r}
hist(dat$hwy,
  xlab = "hwy",
  main = "Histogram of hwy",
  breaks = sqrt(nrow(dat))
)
```

```{r}
library(ggplot2)

ggplot(dat) +
  aes(x = hwy) +
  geom_histogram(bins = 30L, fill = "#0c4c8a") +
  theme_minimal()
```

- Según el histograma, parece que hay un par de observaciones más altas que todas las demás

### Boxplot

- Además de los histogramas, los `boxplots` también son útiles para detectar posibles valores atípicos

```{r}
boxplot(dat$hwy,
  ylab = "hwy"
)
```

```{r}
ggplot(dat) +
  aes(x = "", y = hwy) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()
```

- Un diagrama de caja ayuda a visualizar una variable cuantitativa mostrando cinco resúmenes de localización comunes (mínimo, mediana, primer y tercer cuartil y máximo) y cualquier observación que se haya clasificado como presunto valor atípico utilizando el criterio del rango intercuartílico `(IQR)`. 

- Las observaciones consideradas como posibles valores atípicos según el criterio `IQR` se muestran como puntos en el diagrama de caja. Según este criterio, hay 2 valores atípicos potenciales (véanse los 2 puntos por encima de la línea vertical, en la parte superior del `boxplot`).

- Recuerde que no por el hecho de que una observación sea considerada como un valor atípico potencial por el criterio IQR debe eliminarla. Eliminar o mantener un valor atípico depende de (i) el contexto de su análisis, (ii) si las pruebas que va a realizar en el conjunto de datos son robustas a los valores atípicos o no, y (iii) a qué distancia está el valor atípico de otras observaciones.

- También es posible extraer los valores de los posibles valores atípicos basándose en el criterio `IQR` gracias a la función `boxplot.stats()$out`:

```{r}
boxplot.stats(dat$hwy)$out
```

- Como puede ver, en realidad hay 3 puntos considerados como posibles valores atípicos: 2 observaciones con un valor de 44 y 1 observación con un valor de 41. Gracias a la función `which()` es posible extraer el número de fila correspondiente a estos valores atípicos:

```{r}
out <- boxplot.stats(dat$hwy)$out
out_ind <- which(dat$hwy %in% c(out))
out_ind
```

- Con esta información, ahora puede volver fácilmente a las filas específicas del conjunto de datos para verificarlas, o imprimir todas las variables para estos valores atípicos:

```{r}
dat[out_ind, ]
```

- También es posible imprimir los valores de los valores atípicos directamente en el boxplot con la función `mtext()`

```{r}
boxplot(dat$hwy,
        ylab = "hwy",
        main = "Boxplot of highway miles per gallon"
)
mtext(paste("Outliers: ", paste(out, collapse = ", ")))
```

### Percentiles

- Este método de detección de valores atípicos se basa en los percentiles. Con el método de los percentiles, todas las observaciones que se encuentren fuera del intervalo formado por los percentiles 2.5 y 97.5 se considerarán como posibles valores atípicos. También pueden considerarse otros percentiles, como el 1 y el 99, o el 5 y el 95, para construir el intervalo.

$$
I=[q_{0.25}-1.5\cdot\textsf{IQR}; q_{0.75}+1.5\cdot\textsf{IQR}]
$$

- Los valores de los percentiles inferior y superior (y, por tanto, los límites inferior y superior del intervalo) pueden calcularse con la función `quantile()`

```{r}
lower_bound <- quantile(dat$hwy, 0.025)
lower_bound
```

```{r}
upper_bound <- quantile(dat$hwy, 0.975)
upper_bound
```

- Según este método, todas las observaciones por debajo de 14 y por encima de 35,175 se considerarán posibles valores atípicos. Los números de fila de las observaciones fuera del intervalo pueden extraerse entonces con la función `which()`

```{r}
outlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)
outlier_ind
```

```{r}
dat[outlier_ind, "hwy"]
```

```{r}
dat[outlier_ind, ]
```

- Hay 11 valores atípicos potenciales según el método de los percentiles. Para reducir este número, puede establecer los percentiles en 1 y 99:

```{r}
lower_bound <- quantile(dat$hwy, 0.01)
upper_bound <- quantile(dat$hwy, 0.99)

outlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)

dat[outlier_ind, ]
```

### Filtro de Hampel

- Otro método, conocido como *filtro de Hampel*, consiste en considerar como valores atípicos los valores fuera del intervalo formado por la mediana, más o menos 3 desviaciones absolutas de la mediana

$$
I=[\tilde{X}-3\cdot\textsf{MAD}, \tilde{X}+3\cdot\textsf{MAD}],
$$
  donde `MAD` es la desviación absoluta de la mediana y se define como la mediana de las desviaciones absolutas de la mediana $\tilde{X}=\textsf{median}(X)$ de los datos

$$
\textsf{MAD}=\textsf{median}(|X_{i}-\tilde{X}|)
$$
- Para este método, primero establecemos los límites del intervalo, gracias a las funciones `median()` y `mad()`

```{r}
lower_bound <- median(dat$hwy) - 3 * mad(dat$hwy, constant = 1)
lower_bound
```


```{r}
upper_bound <- median(dat$hwy) + 3 * mad(dat$hwy, constant = 1)
upper_bound
```

- Según este método, todas las observaciones por debajo de 9 y por encima de 39 se considerarán como posibles valores atípicos. Los números de fila de las observaciones que están fuera del intervalo pueden entonces extraerse con la función `which()`. Según el filtro de `Hampel`, hay 3 valores atípicos para la variable `hwy`.

```{r}
outlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)
outlier_ind
```

### Prueba de Grubbs

- La prueba de `Grubbs` permite detectar si el valor más alto o más bajo de un conjunto de datos es un valor atípico. La prueba de `Grubbs` detecta un valor atípico cada vez (valor más alto o más bajo), por lo que las hipótesis nula y alternativa son las siguientes

  - $H_{0}$: El valor más alto/bajo no es un valor atípico
  - $H_{1}$: El valor más alto/bajo es un valor atípico

- Como para cualquier prueba estadística, si el valor $p$ es inferior al umbral de significación elegido (generalmente $\alpha=0.05$), se rechaza la hipótesis nula y se concluye que el valor más bajo/más alto es un valor atípico. Por el contrario, si el valor $p$ es mayor o igual que el nivel de significación, no se rechaza la hipótesis nula y concluiremos que, basándonos en los datos, no rechazamos la hipótesis de que el valor más bajo/más alto no es un valor atípico. Tenga en cuenta que la prueba de `Grubbs` no es apropiada para un tamaño de muestra $n\leq6$. Para realizar la prueba de `Grubbs` en `R`, utilizamos la función `grubbs.test()` del paquete `outliers`


```{r}
library(outliers)
test <- grubbs.test(dat$hwy)
test
```

- El valor $p$ es de 0.056. Al nivel de significación del 5%, no rechazamos la hipótesis de que el valor más alto 44 no es un valor atípico. Por defecto, la prueba se realiza sobre el valor más alto (como se muestra en la salida de `R`: hipótesis alternativa: el valor más alto 44 es un valor atípico). Si desea realizar la prueba para el valor más bajo, simplemente añada el argumento `opposite = TRUE` en la función `grubbs.test()`

```{r}
test <- grubbs.test(dat$hwy, opposite = TRUE)
test
```

- La salida de `R` indica que la prueba se realiza ahora sobre el valor más bajo (véase la hipótesis alternativa: el valor más bajo 12 es un valor atípico). El valor $p$ es 1. Al nivel de significación del 5%, no rechazamos la hipótesis de que el valor más bajo 12 no es un valor atípico.

  - A modo de ilustración, sustituiremos ahora una observación por un valor más extremo y realizaremos la prueba de `Grubbs` en este nuevo conjunto de datos. Reemplacemos el $34^{\text{th}}$ por un valor de 212

```{r}
dat[34, "hwy"] <- 212
```

- Y ahora aplicamos la prueba de `Grubbs` para comprobar si el valor más alto es un valor atípico

```{r}
test <- grubbs.test(dat$hwy)
test
```

- El valor $p$ es < 0.001. Al nivel de significación del 5%, concluimos que el valor más alto 212 es un valor atípico.

### Prueba de Dixon

- Al igual que la prueba de *Grubbs*, la prueba de *Dixon* se utiliza para comprobar si un único valor bajo o alto es un valor atípico. Por lo tanto, si se sospecha que hay más de un valor atípico, la prueba tiene que realizarse en estos valores atípicos sospechosos individualmente.

- Tenga en cuenta que la prueba de `Dixon` es más útil para muestras de pequeño tamaño (normalmente $n\leq 25). Para realizar la prueba de `Dixon` en `R`, utilizamos la función `dixon.test()` del paquete `outliers`. Sin embargo, restringimos nuestro conjunto de datos a las `20` primeras observaciones, ya que la prueba de `Dixon` sólo se puede realizar en muestras de pequeño tamaño (`R` arrojará un error y sólo acepta conjuntos de datos de 3 a 30 observaciones)

```{r}
subdat <- dat[1:20, ]
test <- dixon.test(subdat$hwy)
test
```

- Los resultados muestran que el valor más bajo, 15, es un valor atípico (valor $p = 0.007$). Para comprobar el valor más alto, basta con añadir el argumento `opuesto = TRUE` a la función `dixon.test()`

```{r}
test <- dixon.test(subdat$hwy, opposite = TRUE)
test
```

- Los resultados muestran que el valor más alto 31 no es un valor atípico (valor $p = 0.858$). Es una buena práctica comprobar siempre los resultados de la prueba estadística de valores atípicos con el diagrama de caja para asegurarse de que hemos comprobado todos los valores atípicos potenciales

```{r}
out <- boxplot.stats(subdat$hwy)$out
boxplot(subdat$hwy, ylab = "hwy")
mtext(paste("Outliers: ", paste(out, collapse = ", ")))
```

- A partir del `boxplot`, vemos que también podríamos aplicar la prueba de `Dixon` sobre el valor 20 además del valor 15 realizado anteriormente. Esto puede hacerse encontrando el número de fila del valor mínimo, excluyendo este número de fila del conjunto de datos y aplicando finalmente la prueba de `Dixon` a este nuevo conjunto de datos

```{r}
remove_ind <- which.min(subdat$hwy)
subsubdat <- subdat[-remove_ind, ]

test <- dixon.test(subsubdat$hwy)
test
```

- Los resultados muestran que el segundo valor más bajo, 20, no es un valor atípico (valor $p$ = 0.13).


### Prueba de Rosner

- La prueba de `Rosner` para detectar valores atípicos tiene las siguientes ventajas. Se utiliza para detectar varios valores atípicos a la vez (a diferencia de la prueba de `Grubbs` y `Dixon`, que debe realizarse de forma iterativa para detectar múltiples valores atípicos), y está diseñado para evitar el problema del enmascaramiento, en el que un valor atípico cercano a otro atípico puede pasar desapercibido. A diferencia de la prueba de `Dixon`, hay que tener en cuenta que la prueba de `Rosner` es más apropiada cuando el tamaño de la muestra es grande ($n\geq 20$). Por tanto, volvemos a utilizar el conjunto de datos inicial `dat`, que incluye 234 observaciones.

- Para realizar la prueba de `Rosner` utilizamos la función `rosnerTest()` del paquete `EnvStats`. Esta función requiere al menos 2 argumentos: los datos y el número de presuntos valores atípicos `k` (con `k = 3` como número de presuntos valores atípicos por defecto). Para este ejemplo, establecemos que el número de presuntos valores atípicos sea igual a 3, tal y como sugiere el número de posibles valores atípicos esbozado en el `boxplot` al principio del artículo.

```{r}
library(EnvStats)
test <- rosnerTest(dat$hwy, k = 3)
test
```

- Los resultados interesantes se ofrecen en la tabla `all.stats`

```{r}
test$all.stats
```

- Basándonos en la prueba de `Rosner`, vemos que sólo hay un valor atípico (véase la columna de valores atípicos), y que es la observación `34` (véase `Obs.Num`) con un valor de `212` (véase `Value`).

### Tratamiento de los valores atípicos

- Una vez identificados los valores atípicos y habiendo decidido enmendar la situación según la naturaleza del problema, puede considerar uno de los siguientes enfoques.

- **Imputación**

  Este método se ha tratado en detalle en la discusión sobre el tratamiento de los valores faltantes.

- **Capping**

  Para los valores que se encuentran fuera de los límites de $15\cdot\mathsf{IQR}$, podríamos poner un tope sustituyendo las observaciones que se encuentran fuera del límite inferior por el valor del $5^{\textsf{th}}$ percentile y las que se encuentran por encima del límite superior, por el valor del $95^{\textsf{th}}$ percentile. A continuación se muestra un código de ejemplo que logra esto
  
```{r}
x <- dat$hwy
qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
caps <- quantile(x, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(x, na.rm = T)
x[x < (qnt[1] - H)] <- caps[1]
x[x > (qnt[2] + H)] <- caps[2]
head(x)
```

- **Predicción**
  
  En otro enfoque, los valores atípicos pueden sustituirse por valores perdidos `(NA)` y luego pueden predecirse considerándolos como una variable de respuesta. Ya hemos hablado de cómo predecir los valores perdidos en la sección anterior.
  

# Introducción a PostgreSQL en R

- Cuando se trata de grandes conjuntos de datos que potencialmente exceden la memoria de su máquina, es bueno tener otra posibilidad, como su propio servidor con una base de datos `SQL/PostgreSQL`, donde se puede consultar los datos. Por ejemplo, un conjunto de datos financieros de 5 GB caben en una memoria RAM básica, pero los datos consumen muchos recursos. Una solución es utilizar una base de datos basada en `SQL`, donde puedo consultar los datos en trozos más pequeños, dejando recursos para el cálculo.

- Aunque `MySQL` es la más utilizada, `PostgreSQL` tiene la ventaja de ser de código abierto y gratuita para todos los usos. Sin embargo, todavía tenemos que conseguir un servidor. Una forma posible de hacerlo es alquilar un servidor de `Amazon`, sin embargo, existe la opción de utilizar `Heroku Postgres` tal como se explicó en la sección de `Python`. 

## Conexión con R

- Ahora es el momento de conectarse a la base de datos con `R`. Este enfoque utiliza el paquete `RPostgreSQL`. Los siguientes paquetes y herramientas deben ser instaladas para poder hacer uso de la `API` y realizar la conexión con éxito

  - RPostgresql
  - DBI
  - Su propia base de datos PostgreSQL
  - Acceso remoto a su base de datos

- El paquete `RPostgreSQL` y `DBI` se puede instalar desde `CRAN` o `Github`

```{r eval = FALSE}
install.packages("RPostgreSQL")
install.packages("DBI")
devtools::install_git('https://github.com/r-dbi/DBI.git')
devtools::install_git('https://github.com/cran/RPostgreSQL.git)
```

- Para conectar, necesitamos introducir los siguientes comandos en `R`, nos conectaremos a la base de datos creada anteriormente en `Heroku Postgres` en la sección  de `Python`. RPostgres es una interfaz compatible con `DBI` para la base de datos `Postgres`. Es una reescritura desde cero usando `C++` y `Rcpp`. Este paquete actúa tanto como el controlador de la base de datos como la interfaz `DBI`. El código y la información adicional están disponibles en su repositorio GitHub aquí: [RPostgres](https://github.com/r-dbi/RPostgres)

```{r}
library(DBI)
library(RPostgreSQL)

con <- dbConnect(RPostgres::Postgres(), 
                dbname = "d4oenn2g04nlqm", 
                host = "ec2-34-205-14-168.compute-1.amazonaws.com", 
                port = 5432, 
                user = "xoihbshcjgwwtd", 
                password = "aac4c765bf11a3982c859f19dcd57848c55b6e7b713fb98ca05e3711f2b9d7e9")
```


- Para visualizar la lista de tablas que hemos importado antes en la base de datos utilizamos `dbListTables()`

```{r}
dbListTables(conn = con)
```

## Importación de tablas

- Carguemos como `dataframe` un archivo `CSV` que contiene los precios de las acciones de `Tecnoglass Inc. (TGLS)`, empresa `Barranquillera` que cotiza en `Nasdaq`. Para esto usamos la función `read.csv()`

```{r}
TGLS_df <- read.csv(file = 'TGLS.csv')
head(TGLS_df)
```

- Los nombres de las columnas de este dataframe son problemáticos para las bases de datos (y especialmente para `PostgreSQL`) por varias razones: los `"."` en los nombres pueden ser un problema, y `PostgreSQL` espera que los nombres de las columnas estén todos en minúsculas. Aquí hay una función para hacer que los nombres de las columnas sean seguros para las bases de datos. Luego obtenemos algunos estadísticos usando la función `summary()`

```{r}
dbSafeNames = function(names) {
  names = gsub('[^a-z0-9]+','_',tolower(names))
  names = make.names(names, unique=TRUE, allow_=TRUE)
  names = gsub('.','_',names, fixed=TRUE)
  names
}
colnames(TGLS_df) = dbSafeNames(colnames(TGLS_df))
head(TGLS_df)
```

```{r}
summary(TGLS_df)
```
- Ahora procedemos a insertar tabla `TGLS_df` en nuestra base de datos `Heroku Postgres` usando la función `dbWriteTable()`

```{r}
dbWriteTable(con, 'tecnoglass', TGLS_df, row.names=FALSE, overwrite=TRUE)
```

- La función `dbWriteTable()` devuelve `TRUE` si la tabla fue escrita con éxito. Tenga en cuenta que esta llamada fallará si ya existe la tabla en la base de datos. Utilice `overwrite = TRUE` para forzar la sobreescritura de una tabla existente, y `append = TRUE` para añadirla a una tabla existente. Verifique que la tabla fué creada usando `pgAdmin`

![](figures/postgres1.png)

- Ahora puedes volver a leer la tabla usando `dbGetQuery()` o `dbReadTable`

```{r}
dtab = dbGetQuery(con, "SELECT * FROM tecnoglass")
summary(dtab)
```

```{r}
rm(dtab)
dtab = dbReadTable(con, "tecnoglass")
summary(dtab)
```

- Por supuesto, el objetivo de utilizar una base de datos es extraer subconjuntos o transformaciones de sus datos, utilizando `SQL`. Para esto procedemos de la siguiente forma

```{r}
rm(dtab)
dtab = dbGetQuery(con, "SELECT date, open, close, high, low FROM tecnoglass")
head(dtab)
```

- Puedes utilizar `dbSendQuery` para enviar consultas que no devuelven un resultado tipo marco de datos. Luego asegúrese de enviar los cambios a la base de datos usando `dbCommit`

```{r eval=FALSE}
dbBegin(con)
dbSendQuery(con, "DROP TABLE IF EXISTS tecnoglass")
dbCommit(con)
```

- Cuando hayas terminado, desconecta

```{r}
dbDisconnect(con)
```

## Candlesticks plot

- Usaremos los datos importados en la base de datos para realizar grafico de velas japonesas usando [plotly](https://plotly.com/r/candlestick-charts/). Inciamos realizando la conexión a nuestra base de datos, luego realizamos la figura utilizando la función `plot_ly()` usando `type="candlestick"`


```{r}
library(DBI)
library(RPostgreSQL)

con <- dbConnect(RPostgres::Postgres(), 
                dbname = "d4oenn2g04nlqm", 
                host = "ec2-34-205-14-168.compute-1.amazonaws.com", 
                port = 5432, 
                user = "xoihbshcjgwwtd", 
                password = "aac4c765bf11a3982c859f19dcd57848c55b6e7b713fb98ca05e3711f2b9d7e9")
```


```{r}
dtab = dbGetQuery(con, "SELECT * FROM tecnoglass")
head(dtab)
```

```{r}
library(plotly)


fig <- dtab %>% plot_ly(x = ~date, type="candlestick",
                        open = ~open, close = ~close,
                        high = ~high, low = ~low) 
fig <- fig %>% layout(title = "Basic Candlestick Chart",
                      xaxis = list(title = 'Day'),
                      yaxis = list(title = 'TGLS-USD'))
fig
```


```{r}
i <- list(line = list(color = '#FFD700'))
d <- list(line = list(color = '#0000ff'))

fig <- dtab %>% plot_ly(x = ~date, type="ohlc",
                        open = ~open, close = ~close,
                        high = ~high, low = ~low,
                        increasing = i, decreasing = d) 
fig <- fig %>% layout(title = "Basic OHLC Chart",
                      xaxis = list(rangeslider = list(visible = F), title = 'Day'),
                      yaxis = list(title = 'TGLS-USD'))

fig
```

```{r}
dbDisconnect(con)
```

