[["index.html", "Visualización de Datos con R Sección 3 Chapter 1 Modelo de series de tiempo 1.1 Modelo ARIMA 1.2 El proceso estacionario 1.3 Transformación de una serie no estacionaria en una serie serie estacionaria 1.4 Diferenciación de series temporales 1.5 Transformación logarítmica 1.6 Proceso ARIMA 1.7 Identificación del grado de diferenciación del modelo 1.8 Predicción del consumo mensual de gas natural en EE.UU 1.9 La función auto.arima", " Visualización de Datos con R Sección 3 Lihki Rubio 2021-10-25 Chapter 1 Modelo de series de tiempo En esta sección estudiaremos la forma de implementar el modelo ARIMA estudiado antes en la sección de Python. La teoría abordada para este modelo es la misma, por lo tanto procedemos a la implementación en R de este modelo predictivo 1.1 Modelo ARIMA El modelo de Media Móvil Integrada Autorregresiva (ARIMA) es el nombre genérico de una familia de modelos predictivos que se basan en los procesos Autorregresivo (AR) y de Media Móvil (MA). Entre los modelos predictivos tradicionales (por ejemplo, la regresión lineal, suavización exponencial, etc.), el modelo ARIMA se considera el enfoque más avanzado y robusto. En esta sección, presentaremos los componentes del modelo: los procesos AR y MA y el componente de diferenciación. Además, nos centraremos en los métodos y enfoques para ajustar los parámetros del modelo con el uso de diferenciación, la función de autocorrelación (ACF) y la función de autocorrelación parcial (PACF). En esta sección cubriremos lo siguiente El estado estacionario de los datos de las series temporales El proceso random walk Los procesos AR y MA Los modelos ARMA y ARIMA El modelo ARIMA estacional Librerías: forecast TSstudio plotly dplyr lubridate stats datasets base 1.2 El proceso estacionario Uno de los principales supuestos de la familia de modelos ARIMA es que las series de entrada siguen la estructura de un proceso estacionario. Este supuesto se basa en el Teorema de representación de Wold, que afirma que cualquier proceso estacionario puede representarse como una combinación lineal de ruido blanco. Por lo tanto, antes de sumergirnos en los componentes del modelo ARIMA, vamos a detenernos a hablar del proceso estacionario. El proceso estacionario, en el contexto de los datos de las series temporales, describe un estado estocástico de la serie. Los datos de las series temporales son estacionarios si se dan las siguientes condiciones: La media y la varianza de la serie no cambian con el tiempo La estructura de correlación de la serie, junto con sus rezagos, permanece igual a lo largo del tiempo tiempo En los siguientes ejemplos, utilizaremos la función arima.sim del paquete stats para simular unos datos de series temporales estacionarios y no estacionarios y los representaremos con la función ts_plot del paquete TSstudio. La función arima.sim nos permite simular datos de series temporales basándonos en los componentes y características principales del modelo ARIMA: Un proceso autorregresivo (AR): Establece una relación entre la serie y sus \\(p\\) rezagos pasados con el uso de un modelo de regresión Un proceso de media móvil (MA): Similar al proceso AR, el proceso MA establece la relación con el término de error en el tiempo \\(t\\) y los términos de error pasados, con el uso de regresión entre los dos componentes Proceso integrado (I): El proceso de diferenciar la serie con sus \\(d\\) rezagos para transformar la serie en un estado estacionario. Aquí, el argumento del modelo de la función define \\(p, q\\) y \\(d\\), así como el orden de los procesos AR, MA, y los procesos I del modelo. Consideraríamos una serie temporal de datos como no estacionaria siempre que las condiciones mencionadas anteriormente no se cumplan. Los ejemplos más comunes de una serie con una estructura no estacionaria son los siguientes: Una serie con una tendencia dominante: La media de la serie cambia a lo largo del tiempo como función del cambio en la tendencia de la serie, y por tanto la serie es no estacionaria Una serie con un componente estacional multiplicativo: En este caso, la varianza de la serie es una función de la oscilación estacional a lo largo del tiempo, que aumenta o disminuye con el tiempo La serie clásica AirPassenger (el número mensual de pasajeros de las aerolíneas entre 1949 y 1960) del paquete de conjuntos de datos es un buen ejemplo de una serie que viola las dos condiciones del proceso estacionario. Dado que la serie tiene tanto una fuerte tendencia lineal como un estacional multiplicativo, tanto la media como la varianza cambian con el tiempo: library(TSstudio) data(AirPassengers) ts_plot(AirPassengers, title = &quot;Monthly Airline Passenger Numbers 1949-1960&quot;, Ytitle = &quot;Thousands of Passengers&quot;, Xtitle = &quot;Year&quot;) 1.3 Transformación de una serie no estacionaria en una serie serie estacionaria En la mayoría de los casos, a menos que tenga mucha suerte, sus datos brutos probablemente vendrán con una tendencia u otra forma de oscilación que viole los supuestos del proceso estacionario. Por lo tanto, para manejar esto, tendrá que aplicar algunos pasos de transformación con el fin de llevar la serie a un estado estacionario. Los métodos de transformación más comunes son la diferenciación la serie (o la des-tendencia) y la transformación logarítmica (o ambas). Repasemos las aplicaciones de estos métodos. 1.4 Diferenciación de series temporales El método más habitual para transformar los datos de una serie temporal no estacionaria en un estado estacionario es diferenciar la serie con sus rezagos. El principal efecto de diferenciar una serie es la eliminación de la tendencia de la serie (o la pérdida de tendencia de la serie), que ayuda a estabilizar la media de la serie. Medimos el grado u orden de diferenciación de la serie por el número de veces que diferenciamos la serie con sus rezagos. Por ejemplo, la siguiente ecuación define la diferencia de primer orden: \\[ Y_{t}&#39;=Y_{t}-Y_{t-1} \\] Aquí \\(Y_{t}&#39;\\), representa la diferencia de primer orden de la serie, y \\(Y_{t}, Y_{t-1}\\) representan la serie y su primer rezago respectivamente. En algunos casos, el uso de la diferencia de primer orden no es suficiente para llevar la serie a un estado estacionario, y es posible que desee aplicar la diferencia de segundo orden: \\[ Y_{t}^{*}=Y_{t}^{&#39;}-Y_{t-1}^{&#39;}=(Y_{t}-Y_{t-1})-(Y_{t-1}-Y_{t-2})= Y_{t}-2Y_{t-1}+Y_{t-2} \\] - Otra forma de diferenciación es la diferenciación estacional, que se basa en diferenciar la series con el desfase estacional: \\[ Y_{t}^{&#39;}=Y_{t}-Y_{t-f} \\] Aquí, \\(f\\) representa la frecuencia de la serie y \\(Y_{t-f}\\) representa el rezago estacional de la serie. La función diff del paquete base diferencia la serie de entrada con un rezago específico, configurando el argumento lag de rezago correspondiente. Volvamos a la serie serie AirPassenger y veamos cómo el primer orden y la diferenciación estacional afectan a la estructura de la serie. Empezaremos con la diferencia de primer orden: library(TSstudio) data(AirPassengers) ts_plot(diff(AirPassengers, lag = 1), title = &quot;AirPassengers Series - First Differencing&quot;, Xtitle = &quot;Year&quot;, Ytitle = &quot;Differencing of Thousands of Passengers&quot;) Puede ver que la primera diferencia de la serie AirPassenger eliminó la tendencia de la serie y que la media de la serie es, en general, constante en el tiempo. Por otra parte, hay una clara evidencia de que la variación de la serie está aumentando con el tiempo, y por lo tanto la serie aún no es estacionaria. Además de la diferencia de primer orden, tomar la diferencia estacional de la serie podría resolver este problema. Añadamos la diferencia estacional a la diferencia de primer orden y trazémosla de nuevo: library(TSstudio) data(AirPassengers) ts_plot(diff(diff(AirPassengers, lag = 1), 12), title = &quot;AirPassengers Series - First and Seasonal Differencing&quot;, Xtitle = &quot;Year&quot;, Ytitle = &quot;Differencing of Thousands of Passengers&quot;) La diferencia estacional ha conseguido estabilizar la variación de la serie, ya que ahora la serie parece ser estacionaria. 1.5 Transformación logarítmica Podemos utilizar transformación logarítmica para estabilizar una oscilación estacional multiplicativa, si es que existe. Este enfoque no es un sustitución de la diferenciación, sino una adición. Por ejemplo, en el caso de AirPassenger de la sección anterior, vimos que la primera diferenciación hace un gran trabajo al estabilizar la media de la serie, pero no es suficiente para estabilizar la varianza de la serie. Por lo tanto, podemos aplicar una transformación logarítmica para transformar la estructura estacional de multiplicable a aditiva y luego aplicar la diferencia de primer orden para estacionar la serie: ts_plot(diff(log(AirPassengers), lag = 1), title = &quot;AirPassengers Series - First Differencing with Log Transformation&quot;, Xtitle = &quot;Year&quot;, Ytitle = &quot;Differencing/Log of Thousands of Passengers&quot;) La transformación logarítmica con la diferenciación de primer orden está haciendo un mejor trabajo de transformación de la serie en un estado estacionario con respecto a la doble diferenciación (de primer orden con diferenciación estacional) que utilizamos anteriormente. 1.6 Proceso ARIMA Una de las limitaciones de los modelos AR, MA y ARMA es que no pueden manejar datos de series temporales no estacionarias. Por lo tanto, si la serie de entrada es no estacionaria, se requiere un paso de preprocesamiento para transformar la serie de un estado no estacionario a un estado estacionario. El modelo ARIMA ofrece una solución para este problema al añadir el proceso integrado para el modelo ARMA. El proceso integrado (I) consiste simplemente en diferenciar la serie con sus rezagos, donde el grado de diferenciación está representado por el parámetro d. Podemos generalizar el proceso de diferenciación con la siguiente ecuación: \\[ Y_{d}=(Y_{t}-Y_{t-1})-\\cdots-(Y_{t-d+1}-Y_{t-d}), \\] donde \\(Y_{d}\\) es la \\(d\\) diferenciación de la series. Añadamos el componente de diferenciación al modelo ARMA y formalicemos el modelo ARIMA: \\[ \\textsf{ARIMA}(p, d, q): Y_{d}=c+\\sum_{i=1}^{p}\\phi_{i}Y_{d-i}+\\sum_{i=1}^{q}\\theta_{i}\\epsilon_{t-i}+\\epsilon_{t}, \\] donde ARIMA\\((p,d,q)\\) define un proceso ARIMA con un proceso AR de orden \\(p\\), un grado \\(d\\) de diferenciación, y un proceso MA de orden \\(q\\) \\(Y_{d}\\) es la diferencia \\(d\\) de la serie \\(Y_{t}\\) \\(c\\) representa una constante (o desviación) \\(p\\) define el número de rezagos que se regresan contra \\(Y_{t}\\) \\(Y_{d-i}\\) es el coeficiente del rzago \\(i\\) de la serie \\(q\\) define el número de términos de error pasados que se utilizarán en la ecuación \\(\\theta_{i}\\) es el coeficiente correspondiente de \\(\\epsilon_{t-i}\\) \\(\\epsilon_{t-q},\\dots,\\epsilon_{t}\\) son términos de error de ruido blanco \\(\\epsilon_{t}\\) representa el término de error, que es ruido blanco Se pueden representar los modelos AR, MA o ARMA con el modelo ARIMA, por ejemplo: El modelo ARIMA(0, 0, 0) es equivalente al ruido blanco El modelo ARIMA(0, 1, 0) es equivalente a un paseo aleatorio El modelo ARIMA(1, 0, 0) es equivalente a un proceso AR(1) El modelo ARIMA(0, 0, 1) es equivalente a un proceso MA(1) El modelo ARIMA(1, 0, 1) es equivalente a un proceso ARMA(1,1) 1.7 Identificación del grado de diferenciación del modelo Al igual que los parámetros \\(p\\) y \\(q\\), el ajuste del parámetro \\(d\\) (el grado de diferenciación de la de la serie) puede hacerse con los gráficos ACF y PACF. En el siguiente ejemplo, utilizaremos los precios mensuales del café Robusta desde el año 2000. Esta serie forma parte del objeto Coffee_Prices del paquete TSstudio. Empezaremos cargando la serie Coffee_Prices y restando los precios mensuales del café Robusta desde enero de 2010 utilizando la función window: data(&quot;Coffee_Prices&quot;) robusta_price &lt;- window(Coffee_Prices[,1], start = c(2000, 1)) ts_plot(robusta_price, title = &quot;The Robusta Coffee Monthly Prices&quot;, Ytitle = &quot;Price in USD&quot;, Xtitle = &quot;Year&quot;) Como se puede ver, los precios del café Robusta a lo largo del tiempo tienen una tendencia al alza, por lo que no se encuentra en un estado estacionario. Además, como esta serie representa precios continuos, es probable que la serie tenga una fuerte relación de correlación con sus rezagos pasados (ya que los cambios en el precio son típicamente cercanos al precio anterior). Volveremos a utilizar la función acf para identificar el tipo de relación entre la serie y sus rezagos: acf(robusta_price) Como se puede ver en la salida anterior del gráfico ACF, la correlación de la serie con sus rezagos decae lentamente en el tiempo de forma lineal. La eliminación de la tendencia de la serie y de la correlación entre la serie y sus rezagos puede hacerse por diferenciación. Comenzaremos Comenzaremos con la primera diferenciación utilizando la función diff: robusta_price_d1 &lt;- diff(robusta_price) par(mfrow=c(1,2)) acf(robusta_price_d1) pacf(robusta_price_d1) Los gráficos ACF y PACF de la primera diferencia de la serie indican que un proceso AR(1)es apropiado para utilizar en la serie diferenciada, ya que el ACF es de cola y el PACF corta en el primer rezago. Por lo tanto, aplicaremos un modelo ARIMA(1,1,0) a la serie robusta_price para incluir la primera diferencia: library(forecast) robusta_md &lt;- arima(robusta_price, order = c(1, 1, 0)) Utilizaremos la función summary para revisar los detalles del modelo: summary(robusta_md) ## ## Call: ## arima(x = robusta_price, order = c(1, 1, 0)) ## ## Coefficients: ## ar1 ## 0.2780 ## s.e. 0.0647 ## ## sigma^2 estimated as 0.007142: log likelihood = 231.38, aic = -458.76 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.002595604 0.08432096 0.06494772 0.08104715 4.254984 1.001542 ## ACF1 ## Training set 0.001526295 Puede ver en el resumen del modelo que el coeficiente ar1 es estadísticamente significativo. Por último, pero no menos importante, comprobaremos los residuos del modelo: checkresiduals(robusta_md) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,1,0) ## Q* = 26.896, df = 23, p-value = 0.2604 ## ## Model df: 1. Total lags used: 24 En general, el gráfico de los residuos del modelo y la prueba de Ljung-Box indican que los residuos son white noise. El gráfico ACF indica que hay algunos rezagos correlacionados, pero ellos son sólo significativos ewn los bordes, por lo que podemos ignorarlos. Muchas pruebas estadísticas se utilizan para intentar rechazar alguna hipótesis nula. En este caso concreto, la prueba de Ljung-Box trata de rechazar la independencia de algunos valores Si el valor \\(p &lt; 0,05\\): Puedes rechazar la hipótesis nula asumiendo un 5% de probabilidad de error. Por lo tanto, puede asumir que sus valores muestran dependencia entre sí. Si el valor \\(p &gt; 0,05\\): No tiene suficiente evidencia estadística para rechazar la hipótesis nula. Así que no puede asumir que sus valores son dependientes. Esto podría significar que sus valores son dependientes de todos modos o puede significar que sus valores son independientes. Pero usted no está probando ninguna posibilidad específica, lo que su prueba realmente es que usted no puede afirmar la dependencia de los valores, ni puede afirmar la independencia de los valores. En general, lo importante aquí es tener en cuenta que un valor \\(p &lt; 0.05\\) permite rechazar la hipótesis nula, pero un valor \\(p &gt; 0.05\\) no permite confirmar la hipótesis nula. Esto es, no se puede probar la independencia de los valores de las series temporales utilizando la prueba de Ljung-Box. Sólo puede probar la dependencia. 1.8 Predicción del consumo mensual de gas natural en EE.UU El modelo ARIMA estacional (SARIMA), como su nombre indica, es una versión designada del modelo ARIMA para series temporales con un componente estacional. Una serie temporal con un componente estacional tiene una fuerte relación con sus rezagos estacionales, el modelo SARIMA utiliza los rezagos estacionales de manera similar a como el modelo ARIMA utiliza los rezagos no estacionales con los procesos AR y MA y la diferenciación. Para ello, añade los tres componentes siguientes al modelo ARIMA Proceso SAR(P): Un proceso AR estacional de la serie con sus P rezagos estacionales pasados. Por ejemplo, un SAR(2) es un proceso AR de la serie con sus dos últimos rezagos estacionales, es decir, \\(Y_{t}=c+\\Phi_{1}Y_{t-f}+\\Phi_{2}Y_{t-2f}+\\epsilon_{t}\\) donde \\(\\Phi\\) representa el coeficiente estacional del proceso SAR, y \\(f\\) representa la frecuencia de la serie. Proceso SMA(Q): Un proceso MA estacional de la serie con sus Q términos de error estacionales pasados. Por ejemplo, un SMA(1) es un proceso de media móvil de la serie con su término de error estacional pasado, es decir, \\(Y_{t}=\\mu+\\epsilon_{t}+\\Theta_{1}\\epsilon_{t-f}\\), donde \\(\\Theta\\) representa el coeficiente estacional del proceso SMA, y \\(f\\) representa la frecuencia de la serie. Proceso SI(D): Una diferenciación estacional de la serie con sus últimos D rezagos estacionales. De forma similar, podemos diferenciar la serie con su rezago estacional, es decir \\(Y_{D=1}=Y_{t}-Y_{t-f}\\). Utilizamos la siguiente notación para denotar los parámetros SARIMA, donde los parámetros \\(P\\) y \\(Q\\) representan el orden correspondiente de los procesos AR y MA estacionales de la serie con sus rezagos estacionales, y D define el grado de diferenciación de la serie con sus rezagos no estacionales. \\[ \\textsf{SARIMA}(p, d, q)\\times(P, D, Q), \\] Aplicaremos lo aprendido acerca del modelo ARIMA para pronosticar el consumo mensual de gas natural en Estados Unidos. Vamos a cargar la serie USgas del paquete TSstudio: library(TSstudio) data(USgas) ts_plot(USgas, title = &quot;US Monthly Natural Gas consumption&quot;, Ytitle = &quot;Billion Cubic Feet&quot;, Xtitle = &quot;Year&quot;) Como hemos visto anteriormente, la serie de gas de EE.UU. tiene un fuerte patrón estacional, además la serie tiene una tendencia al alza, por lo tanto, se requiere el uso de los modelos ARIMA. Empezaremos por definir particiones training y testing usando la función ts_split, dejando los últimos 12 meses de la serie como partición de prueba: USgas_split &lt;- ts_split(USgas, sample.out = 12) train &lt;- USgas_split$train test &lt;- USgas_split$test Antes de iniciar el proceso de entrenamiento del modelo SARIMA, realizaremos diagnósticos con respecto a la correlación de la serie con las funciones ACF y PACF. Dado que nos interesa ver la relación de la serie con sus rezagos estacionales, aumentaremos el número de rezagos a calcular y mostrar, estableciendo el argumento lag.max a 60 rezagos par(mfrow=c(1,2)) acf(train, lag.max = 60) pacf(train, lag.max = 60) El gráfico ACF anterior indica que la serie tiene una fuerte correlación tanto con los estacional como con los rezagos no estacionales. Además, el decaimiento lineal de los rezagos estacionales indica que la serie no es estacionaria y que se requiere una diferenciación estacional. Comenzaremos con diferenciación estacional de la serie y trazaremos el resultado para identificar si la serie se encuentra en un estado estacionario: USgas_d12 &lt;- diff(train, 12) ts_plot(USgas_d12, title = &quot;US Monthly Natural Gas consumption - First Seasonal Difference&quot;, Ytitle = &quot;Billion Cubic Feet (First Difference)&quot;, Xtitle = &quot;Year&quot;) Aunque hemos eliminado la tendencia de la serie, la variación de la serie aún no es estable. Por lo tanto, también intentaremos tomar la primera diferencia de la serie: USgas_d12_1 &lt;- diff(diff(USgas_d12, 1)) ts_plot(USgas_d12_1, title = &quot;US Monthly Natural Gas consumption - First Seasonal and Non-Seasonal Differencing&quot;, Ytitle = &quot;Billion Cubic Feet (Difference)&quot;, Xtitle = &quot;Year&quot;) Después de tomar la diferenciación de primer orden, junto con la diferenciación estacional de primer orden, la serie parece estabilizarse en torno a la línea del eje \\(x\\) cero (o bastante cerca de ser estable). Después de transformar la serie en un estado estacionario, podemos revisar las funciones ACF y PACF de nuevo para identificar el proceso necesario par(mfrow=c(1,2)) acf(USgas_d12_1, lag.max = 60) pacf(USgas_d12_1, lag.max = 60) La principal observación de los gráficos ACF y PACF anteriores es que tanto los rezagos no estacionales como los rezagos estacionales (en ambos gráficos) se están reduciendo. Por lo tanto, podemos concluir que después de de diferenciar las series y transformarlas en un estado estacionario, deberíamos aplicar un proceso proceso ARMA para los componentes estacionales y no estacionales del modelo SARIMA 1.9 La función auto.arima Uno de los principales retos de la previsión con la familia de modelos ARIMA es el proceso de ajuste de los modelos. Como hemos visto en este capítulo, este proceso incluye muchos pasos manuales que se requieren para verificar la estructura de las series (estacionarias o no estacionaria), las transformaciones de los datos, el análisis descriptivo con los gráficos ACF y PACF para identificar el tipo de proceso y, finalmente, afinar los parámetros del modelo. Si bien entrenar un modelo ARIMA para una sola serie puede llevar unos minutos, es posible que no se pueda hacer si se tienen docenas de series que pronosticar. La función auto.arima del paquete forecast ofrece una solución a este problema. Este algoritmo automatiza el proceso de ajuste del modelo ARIMA con el uso de métodos para identificar tanto la estructura de la serie (estacionaria o no) como el tipo (estacional o no), y establece los parámetros del modelo en consecuencia. Por ejemplo, podemos utilizar la función para pronosticar USgas: library(forecast) USgas_auto_md &lt;- auto.arima(train) USgas_auto_md ## Series: train ## ARIMA(2,1,1)(2,1,1)[12] ## ## Coefficients: ## ar1 ar2 ma1 sar1 sar2 sma1 ## 0.4301 -0.0372 -0.9098 0.0117 -0.2673 -0.7431 ## s.e. 0.0794 0.0741 0.0452 0.0887 0.0830 0.0751 ## ## sigma^2 estimated as 10446: log likelihood=-1292.83 ## AIC=2599.67 AICc=2600.22 BIC=2623.2 Utilizando los argumentos por defecto de la función auto.arima se obtiene el modelo ARIMA que minimice la puntuación AIC. En este caso, se seleccionó un modelo con una puntuación AIC de 2480,57: Por defecto, la función auto.arima aplica una búsqueda de modelos más corta utilizando un enfoque por pasos para reducir el tiempo de búsqueda. La contrapartida de este enfoque es que el modelo puede pasar por alto algunos modelos que pueden obtener mejores resultados. Podemos improvisar con los resultados de auto.arima ajustando el argumento de búsqueda del modelo. El argumento step-wise, cuando se establece en FALSE, permite establecer una búsqueda más robusta y exhaustiva, con el coste de un mayor tiempo de búsqueda. Esta compensación entre el rendimiento y el tiempo de cálculo puede equilibrarse siempre que se tenga un conocimiento previo de la estructura y las características de la serie. Por ejemplo, volvamos a entrenar el conjunto de entrenamiento de la serie USgas esta vez con la siguiente configuración: Establezca los parámetros de diferenciación d y D en 1. Limite el orden del modelo a siete utilizando el argumento max.order. El argumento max.order define los valores máximos de \\(p+q+P+Q\\), por lo que deberíamos fijarlo en cinco (dado que \\(d\\) y \\(D\\) están fijados en 1). Bajo estas restricciones, busque todas las combinaciones posibles estableciendo el argumento argumento stepwise a FALSE. Establezca el argumento de aproximation en FALSE para obtener cálculos más precisos de los criterios de información: USgas_auto_md2 &lt;- auto.arima(train, max.order = 5, D = 1, d = 1, stepwise = FALSE, approximation = FALSE) USgas_auto_md2 ## Series: train ## ARIMA(1,1,1)(2,1,1)[12] ## ## Coefficients: ## ar1 ma1 sar1 sar2 sma1 ## 0.4247 -0.9180 0.0132 -0.2639 -0.7449 ## s.e. 0.0770 0.0376 0.0894 0.0834 0.0753 ## ## sigma^2 estimated as 10405: log likelihood=-1292.96 ## AIC=2597.91 AICc=2598.32 BIC=2618.08 Utilicemos el modelo entrenado USgas_best_md2 para pronosticar las observaciones correspondientes del del conjunto de testing: USgas_test_fc &lt;- forecast(USgas_auto_md2, h = 12) Evaluamos el rendimiento del modelo con la función de precisión: accuracy(USgas_test_fc, test) ## ME RMSE MAE MPE MAPE MASE ## Training set 6.081099 97.85701 73.36854 0.1298714 3.517097 0.6371821 ## Test set 42.211253 104.79281 83.09943 1.4913412 3.314280 0.7216918 ## ACF1 Theil&#39;s U ## Training set 0.004565602 NA ## Test set -0.049999868 0.3469228 Ahora, utilizaremos la función test_forecast para obtener una visión más intuitiva del rendimiento del modelo del modelo en las particiones de entrenamiento y de prueba: test_forecast(USgas, forecast.obj = USgas_test_fc, test = test) Ahora que hemos cumplido las condiciones anteriores, podemos pasar al último paso del proceso de predicción y generar la predicción final con el modelo seleccionado. Empezaremos por reentrenando el modelo seleccionado en toda la serie: final_md &lt;- arima(USgas, order = c(1,1,1), seasonal = list(order = c(2,1,1))) Antes de pronosticar los próximos 12 meses, verifiquemos que los residuos del modelo satisfacen la condición del modelo: checkresiduals(final_md) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,1,1)(2,1,1)[12] ## Q* = 30.173, df = 19, p-value = 0.04964 ## ## Model df: 5. Total lags used: 24 Observando el gráfico de residuos anterior, se puede ver que los residuos son white noise y se distribuyen normalmente. Además, la prueba de Ljung-Box confirma que no hay autocorrelación con un valor \\(p\\) de 0.12, por lo tanto no podemos rechazar la hipótesis nula de que los residuos son white noise. Estamos listos para empezar. Utilicemos la función de predicción para pronosticar los próximos 12 meses de la serie USgas: USgas_fc &lt;- forecast(final_md, h = 12) Podemos trazar la predicción con la función plot_forecast: plot_forecast(USgas_fc, title = &quot;US Natural Gas Consumption - Forecast&quot;, Ytitle = &quot;Billion Cubic Feet&quot;, Xtitle = &quot;Year&quot;) "],["análisis-con-datos-faltantes.html", "Chapter 2 Análisis con datos faltantes 2.1 Introducción 2.2 Visualización de los datos que faltan 2.3 Tipos de datos faltantes 2.4 Detección de valores atípicos", " Chapter 2 Análisis con datos faltantes 2.1 Introducción Los datos faltantes es uno de los temas que se ignoran en la mayoría de los textos introductorios. Probablemente, parte de la razón por la que esto es así es que todavía abundan muchos mitos sobre el análisis con datos ausentes. Además, algunas de las investigación sobre técnicas de vanguardia es aún relativamente nueva. Una razón más legítima para su ausencia en los textos introductorios es que la mayoría de las metodologías más de principios son bastante complicadas, desde el punto de vista matemático. Sin embargo, la increíble ubicuidad de los problemas relacionados con los datos faltantes en el análisis de datos de la vida real requiere que se aborde el tema. Esta sección sirve como una suave introducción al tema y a una de las técnicas más eficaces para tratarla. Un refrán común sobre el tema es algo así como que la mejor manera de tratar con los datos que faltan es no tenerlos. Es cierto que los datos que faltan son un tema complicado, y hay muchas maneras de hacerlo mal. Es importante no llevar este consejo al extremo, sin embargo, para eludir los problemas de los datos perdidos, algunos han impedido participantes en una encuesta, que por ejemplo, siguen sin responder a todas las preguntas de un formulario. Hay tratamientos para los datos que faltan, pero no hay tratamientos para los datos malos. El tratamiento estándar para el problema de los datos que faltan es reemplazar los datos que faltan por valores no ausentes. Este proceso se denomina imputación. En la mayoría de los casos, el objetivo de la imputación no es recrear el conjunto de datos completo perdido, sino permitir que se realicen estimaciones o inferencias estadísticas válidas a partir de los datos perdidos. Por ello, la eficacia de las diferentes técnicas de imputación no puede evaluarse por su capacidad de recrear los datos con la mayor exactitud posible a partir de un conjunto de datos perdidos simulado, sino que deben juzgarse por su capacidad de apoyar las mismas inferencias estadísticas que se obtendrían del análisis de los datos completos que se extraen del análisis. De este modo, rellenar los datos que faltan es sólo un paso hacia el verdadero objetivo: el análisis. El conjunto de datos imputados rara vez se considera el objetivo final de la imputación. En la práctica, hay muchas formas diferentes de tratar los datos que faltan, algunas son buenas y otras no tanto. Algunas están bien en determinadas circunstancias, pero no en otras. Algunas implican la eliminación de datos perdidos, mientras que otras implican la imputación. Vamos a mencionar brevemente algunos de los métodos más comunes. Sin embargo, el objetivo final de, es iniciarle en lo que a menudo se describe como el estándar de oro de las técnicas de imputación: la imputación múltiple. 2.2 Visualización de los datos que faltan Para demostrar la visualización de patrones de datos ausentes, primero tenemos que crear algunos datos ausentes. Este será también el mismo conjunto de datos sobre el que realizaremos el análisis más adelante en el capítulo. Para mostrar cómo utilizar la imputación múltiple en un escenario semirealista, vamos a crear una versión del conjunto de datos mtcars con algunos valores perdidos: Configuremos seed (para la aleatoriedad determinista), y creemos una variable para mantener nuestro nuevo conjunto de datos. Los datos se extrajeron de la revista Motor Trend US de 1974 y comprenden el consumo de combustible y 10 aspectos del diseño y el rendimiento de 32 automóviles (modelos de 1973 a 1974). set.seed(2) miss_mtcars &lt;- mtcars En primer lugar, vamos a crear siete valores faltantes en drat (alrededor del 20 por ciento), cinco valores faltantes en la columna mpg (alrededor del 15 por ciento), cinco valores faltantes en la columna cyl, tres valores faltantes en wt (alrededor del 10 por ciento), y tres valores faltantes en vs some_rows &lt;- sample(1:nrow(miss_mtcars), 7) miss_mtcars$drat[some_rows] &lt;- NA some_rows &lt;- sample(1:nrow(miss_mtcars), 5) miss_mtcars$mpg[some_rows] &lt;- NA some_rows &lt;- sample(1:nrow(miss_mtcars), 5) miss_mtcars$cyl[some_rows] &lt;- NA some_rows &lt;- sample(1:nrow(miss_mtcars), 3) miss_mtcars$wt[some_rows] &lt;- NA some_rows &lt;- sample(1:nrow(miss_mtcars), 3) miss_mtcars$vs[some_rows] &lt;- NA Ahora, vamos a crear cuatro valores que faltan en qsec, pero sólo para los coches automáticos only_automatic &lt;- which(miss_mtcars$am==0) some_rows &lt;- sample(only_automatic, 4) miss_mtcars$qsec[some_rows] &lt;- NA Ahora, echemos un vistazo al conjunto de datos: head(miss_mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 NA 160 110 3.90 2.620 16.46 NA 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 NA 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 NA 3.460 20.22 1 0 3 1 Ahora vamos a visualizar los datos faltantes. La primera forma en que vamos a visualizar el patrón de los datos faltantes es utilizando la función función md.pattern del paquete mice (que es también el paquete que vamos a utilizar para imputar nuestros datos faltantes). Si no tiene el paquete instálelo antes library(mice) md.pattern(miss_mtcars) ## disp hp am gear carb wt vs qsec mpg cyl drat ## 12 1 1 1 1 1 1 1 1 1 1 1 0 ## 6 1 1 1 1 1 1 1 1 1 1 0 1 ## 2 1 1 1 1 1 1 1 1 1 0 1 1 ## 3 1 1 1 1 1 1 1 1 0 1 1 1 ## 1 1 1 1 1 1 1 1 1 0 0 1 2 ## 1 1 1 1 1 1 1 1 0 1 1 1 1 ## 1 1 1 1 1 1 1 1 0 0 1 1 2 ## 1 1 1 1 1 1 1 0 1 1 0 1 2 ## 1 1 1 1 1 1 1 0 0 1 1 1 2 ## 1 1 1 1 1 1 1 0 0 1 0 1 3 ## 2 1 1 1 1 1 0 1 1 1 1 1 1 ## 1 1 1 1 1 1 0 1 1 1 1 0 2 ## 0 0 0 0 0 3 3 4 5 5 7 27 Un patrón de datos faltantes por fila se refiere a las columnas que faltan en cada fila. Esta función agrega y cuenta el número de filas con el mismo patrón de datos perdidos. Esta función produce una matriz binaria (0 y 1). Las celdas con un 1 representan datos no faltantes; los 0s representan datos que faltan. Como las filas están ordenadas en un orden creciente de ausencia, la primera fila siempre se refiere al patrón de datos ausentes que contiene la menor cantidad de datos faltantes. La columna más a la izquierda es un conteo del número de filas que muestran el patrón de datos faltantes, y la columna más a la derecha es un recuento del número de puntos de datos perdidos en ese patrón. La última fila contiene un recuento del número de puntos de datos que faltan en cada columna. Como puede ver, 12 de las filas no contienen datos perdidos. Sólo hay seis filas que contienen más de un valor perdido. Sólo una de estas filas contiene más de dos valores perdidos (como se muestra en la penúltima fila). En cuanto a los conjuntos de datos con datos faltantes, este en particular no contiene mucho. No es raro que en algunos conjuntos de datos falte más del 30% de los datos. Este conjunto de datos no llega ni al 3%. Ahora vamos a visualizar el patrón de datos faltantes gráficamente utilizando el paquete VIM. Probablemente también tenga que instalarlo. library(VIM) aggr(miss_mtcars, numbers=TRUE) marginplot(miss_mtcars[c(1,2)]) A simple vista, esta representación nos muestra, sin esfuerzo, que la columna drat representa la mayor proporción de faltas, por columnas, seguida de mpg, cyl, qsec, vs. y wt. El gráfico de la derecha nos muestra información similar a la de la salida de md.pattern. Esta representación, sin embargo, hace que sea más fácil saber si hay algún patrón sistemático de omisión. Las celdas azules representan los datos no ausentes, y las rojas representan los datos que faltan. Los números de la derecha del gráfico representan la proporción de filas que muestran ese patrón de datos perdidos. El 37,5% de las filas no contienen ningún tipo de dato que falte. El diagrama de caja rojo de la izquierda muestra la distribución de cyl sin mpg, mientras que el diagrama de caja azul muestra la distribución de los restantes puntos de datos. Lo mismo ocurre con los gráficos de caja de mpg en la parte inferior del gráfico. Si los datos tienen mecanismo MCAR se espera que los gráficos de caja rojos y azules sean muy similares. 2.3 Tipos de datos faltantes El paquete VIM nos permitió visualizar los patrones de datos que faltaban. Un término relacionado, el mecanismo de datos perdidos, describe el proceso que determina la probabilidad de que cada punto de datos sea faltante. Hay tres categorías principales de mecanismos de datos perdidos: Missing Completely At Random (MCAR), Missing At Random (MAR), y Missing Not At Random (MNAR). La discriminación basada en el mecanismo de datos perdidos es crucial, ya que nos informa sobre las opciones para manejar los datos faltantes. Faltantes completamente al azar (MCAR) El primer mecanismo, MCAR, se produce cuando la falta de datos no está relacionada con la datos. Esto ocurriría, por ejemplo, si se borraran filas de una base de datos al azar, o si una ráfaga de viento se llevara una muestra aleatoria de los formularios de encuesta de un investigador. El mecanismo que rige la ausencia de drat, mpg, cyl, wt, y vs es MCAR, porque seleccionamos al azar los elementos que faltan. Este mecanismo, aunque es el más fácil de trabajar, rara vez es sostenible en la práctica. Faltantes no aleatorios (MNAR) El mecanismo MNAR se produce cuando la ausencia de una variable está relacionada con la variable en sí misma. Por ejemplo, supongamos que la báscula que pesa cada coche tiene una capacidad de sólo 3.700 libras y, por ello, los ocho coches que pesaban más se registraron como NA. Este es un ejemplo clásico del mecanismo MNAR, es el propio peso de la observación la causa de que falte. Otro ejemplo sería si durante el transcurso del ensayo de un fármaco antidepresivo los participantes a los que no les ayudaba el fármaco se deprimían demasiado para continuar con el ensayo. Al final del ensayo, cuando se accede y se registra el nivel de depresión de todos los participantes, habrá valores que falten para los participantes cuyo motivo de ausencia está relacionado con su nivel de depresión. Faltantes al azar (MAR) El mecanismo, faltante al azar, tiene un nombre un tanto desafortunado, al contrario de lo que pueda parecer, significa que existe una relación sistemática entre la ausencia de una variable de resultado y otras variables observadas, pero no la variable de resultado en sí misma. La mejor manera de explicarlo es con el siguiente ejemplo: Supongamos que en una encuesta hay una pregunta sobre el nivel de ingresos. Debido a ello, un gran número de participantes en la encuesta cuya lengua materna no es el inglés no pudo interpretar la pregunta y la dejaron en blanco. Si la encuesta sólo recogía el nombre, el sexo y los ingresos, el mecanismo de datos faltantes de la pregunta sobre los ingresos sería MNAR. Sin embargo, si el cuestionario incluía una pregunta sobre si el participante hablaba inglés como como primera lengua, el mecanismo sería MAR. La inclusión de la variable Is English significa que la falta de respuesta a la pregunta sobre los ingresos puede puede explicarse por completo. La razón por la que el nombre falta al azar es que cuando se controla la relación entre la variable omitida y la(s) variable(s) observada(s) con la que está relacionada (por ejemplo, ¿Cuál es su renta? y ¿Es el inglés su primera lengua? respectivamente), los datos faltan al azar. Como otro ejemplo, existe una relación sistemática entre las variables am y qsec en nuestro conjunto de datos simulados que faltan: los qsecs sólo faltan en los coches automáticos. Pero dentro del grupo de coches automáticos, la variable qsec falta al azar. Por lo tanto, el mecanismo de qsec es MAR; controlando el tipo de transmisión, qsec falta al azar. Tenga en cuenta, sin embargo, que si eliminamos am de nuestro conjunto de datos simulado, qsec se convertiría en MNAR. Observación Quizá haya observado que el lugar que ocupa un conjunto de datos concreto en la taxonomía del mecanismo de datos perdidos depende de las variables que incluye. Por ejemplo, sabemos que el mecanismo detrás de qsec es MAR, pero si el conjunto de datos no incluyera am, sería MNAR. Como somos nosotros los que creamos los datos, sabemos el procedimiento que dio lugar a los valores perdidos de qsec. Si no fuéramos nosotros los que creamos los datos, como ocurre en el mundo real, y el conjunto de datos no contuviera la columna am simplemente veríamos una cantidad de valores qsec que faltan arbitrariamente. Esto podría llevarnos a creer que los datos son MCAR. Sin embargo, no lo es; sólo porque la variable con la cual otra variable faltante está sistemáticamente relacionada no se observa, no quiere decir que esta no exista. Esto plantea una cuestión crítica: ¿podemos estar seguros de que nuestros datos no son MCAR? La respuesta desafortunada es no. Dado que los datos que necesitamos para demostrar o refutar el MNAR faltan ipso facto, la suposición de MNAR nunca puede ser desconfirmada de forma concluyente. Es nuestro trabajo, como analistas de datos con pensamiento crítico, preguntar si es probable que haya un mecanismo MNAR o no. 2.3.1 Visualización de datos faltantes Tomando el conjunto de datos airquality, un conjunto de datos de mediciones diarias de la calidad del aire en Nueva York de mayo a septiembre de 1973, que tiene valores NA dentro de sus variables. Las filas del conjunto de datos representan 154 días consecutivos. Cualquier eliminación de estas filas afectará a la continuidad del tiempo, lo que puede afectar a cualquier análisis de series temporales que se realice. Veamos con más detalle el conjunto de datos de la calidad del aire Iniciamos visualizando los datos faltantes. Eliminaremos algunos puntos de datos del conjunto de datos para este ejemplo. En lo que respecta a las variables categóricas, la sustitución de las mismas no suele ser aconsejable. Algunas prácticas comunes incluyen la sustitución de las variables categóricas que faltan por la moda de las observadas, sin embargo, es cuestionable si es una buena opción. Aunque en este caso no faltan puntos de datos de las variables categóricas, las eliminamos de nuestro conjunto de datos (podemos volver a añadirlas más tarde si es necesario) y echamos un vistazo a los datos utilizando summary() data &lt;- airquality data[4:10, 3] &lt;- rep(NA, 7) data[1:5, 4] &lt;- NA data &lt;- data[-c(5,6)] summary(data) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :57.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:73.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.806 Mean :78.28 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 NA&#39;s :7 NA&#39;s :5 Al parecer, el Ozone es la variable con más puntos de datos perdidos. A continuación vamos a profundizar en los patrones de datos que faltan. Suponiendo que los datos sean MCAR, demasiados datos perdidos también pueden ser un problema. Por lo general, un umbral máximo seguro es el 5% del total para conjuntos de datos grandes. Si los datos que faltan para una determinada característica o muestra son superiores al 5%, probablemente deba dejar de lado esa característica o muestra. Por lo tanto, comprobamos las características (columnas) y las muestras (filas) en las que falta más del 5% de los datos mediante una sencilla función pMiss &lt;- function(x){sum(is.na(x))/length(x)*100} apply(data,2,pMiss) ## Ozone Solar.R Wind Temp ## 24.183007 4.575163 4.575163 3.267974 apply(data,1,pMiss) ## [1] 25 25 25 50 100 50 25 25 25 50 25 0 0 0 0 0 0 0 ## [19] 0 0 0 0 0 0 25 25 50 0 0 0 0 25 25 25 25 25 ## [37] 25 0 25 0 0 25 25 0 25 25 0 0 0 0 0 25 25 25 ## [55] 25 25 25 25 25 25 25 0 0 0 25 0 0 0 0 0 0 25 ## [73] 0 0 25 0 0 0 0 0 0 0 25 25 0 0 0 0 0 0 ## [91] 0 0 0 0 0 25 25 25 0 0 0 25 25 0 0 0 25 0 ## [109] 0 0 0 0 0 0 25 0 0 0 25 0 0 0 0 0 0 0 ## [127] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [145] 0 0 0 0 0 25 0 0 0 Vemos que al ozono le falta casi el 25% de los puntos de datos, por lo que podríamos considerar la posibilidad de eliminarlo del análisis o de reunir más mediciones. Las demás variables están por debajo del umbral del 5%, por lo que podemos mantenerlas. En lo que respecta a las muestras, la falta de una sola característica supone un 25% de datos perdidos por muestra. Las muestras a las que les faltan 2 o más características (&gt; 50%) deben ser eliminadas si es posible. Usemos md.pattern() para conocer mejor el patrón de los datos que faltan library(mice) md.pattern(data) ## Temp Solar.R Wind Ozone ## 104 1 1 1 1 0 ## 34 1 1 1 0 1 ## 3 1 1 0 1 1 ## 1 1 1 0 0 2 ## 4 1 0 1 1 1 ## 1 1 0 1 0 2 ## 1 1 0 0 1 2 ## 3 0 1 1 1 1 ## 1 0 1 0 1 2 ## 1 0 0 0 0 4 ## 5 7 7 37 56 La salida nos dice que 104 muestras están completas, que a 34 muestras les falta sólo la medición de Ozone, que a 4 muestras les falta sólo el valor de Solar.R y así sucesivamente. Una representación visual quizás más útil puede obtenerse utilizando el paquete VIM de la siguiente manera library(VIM) aggr_plot &lt;- aggr(data, col=c(&#39;navyblue&#39;,&#39;red&#39;), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c(&quot;Histogram of missing data&quot;,&quot;Pattern&quot;)) ## ## Variables sorted by number of missings: ## Variable Count ## Ozone 0.24183007 ## Solar.R 0.04575163 ## Wind 0.04575163 ## Temp 0.03267974 El gráfico nos ayuda a entender que a casi el 70% de las muestras no les falta ninguna información, al 22% le falta el valor de Ozono y las restantes muestran otros patrones de ausencia. A través de este enfoque, la situación parece un poco más clara. Otra aproximación visual (esperemos) útil es un gráfico de caja especial marginplot(data[c(1,2)]) Obviamente, aquí estamos limitados a trazar sólo 2 variables a la vez, pero sin embargo podemos obtener algunas ideas interesantes. El diagrama de caja rojo de la izquierda muestra la distribución de Solar.R sin Ozono, mientras que el diagrama de caja azul muestra la distribución de los restantes puntos de datos. Lo mismo ocurre con los gráficos de caja de Ozono en la parte inferior del gráfico. Si nuestra suposición de los datos MCAR es correcta, entonces esperamos que los gráficos de caja rojos y azules sean muy similares. 2.3.2 Borrado de la lista El método más utilizado por los científicos de datos para tratar los datos que faltan es simplemente omitir los casos con datos que faltan, analizando únicamente el resto del conjunto de datos. Este método se conoce como eliminación por lista o análisis de casos completos. La función na.omit() en R elimina todos los casos con uno o más valores de datos perdidos en un conjunto de datos. La mayor ventaja de este método es su comodidad. Sin embargo, si la naturaleza de los datos es MCAR, la eliminación de la lista dará lugar a errores estándar que son significativos para los datos reducidos, pero no para todo el conjunto de datos, que tenía los datos que faltaban. Este método de tratar los datos que faltan es posiblemente un desperdicio. Si se eliminan los casos de esta manera, hay que ser consciente de la disminución de la capacidad para detectar el verdadero efecto de las variables de interés. Sin embargo, si los datos no son MCAR, el análisis de casos completos puede influir mucho en las estimaciones de la media, los coeficientes de regresión y las correlaciones. La supresión de la lista puede causar submuestras sin sentido. A continuación, realizamos una simple función na.omit() para eliminar los casos que tienen NAs. Vemos que todas las filas que contenían algún NA en cualquier variable son eliminadas del dataframe airquality_omit &lt;- na.omit(data) head(airquality_omit) ## Ozone Solar.R Wind Temp ## 12 16 256 9.7 69 ## 13 11 290 9.2 66 ## 14 14 274 10.9 68 ## 15 18 65 13.2 58 ## 16 14 334 11.5 64 ## 17 34 307 12.0 66 Dibujemos los histogramas antes y después de la imputación usando ggplot library(ggplot2) library(tidyverse) library(hrbrthemes) library(gridExtra) ggp1 &lt;- ggplot(data.frame(value=data$Ozone), aes(x=value)) + geom_histogram(fill=&quot;#FBD000&quot;, color=&quot;#E52521&quot;, alpha=0.9) + ggtitle(&quot;Original data&quot;) + xlab(&#39;Ozone&#39;) + ylab(&#39;Frequency&#39;) + theme_ipsum() + theme(plot.title = element_text(size=15)) ggp2 &lt;- ggplot(data.frame(value=airquality_omit$Ozone), aes(x=value)) + geom_histogram(fill=&quot;#43B047&quot;, color=&quot;#049CD8&quot;, alpha=0.9) + ggtitle(&quot;Listwise Deletion&quot;) + xlab(&#39;Ozone&#39;) + ylab(&#39;Frequency&#39;) + theme_ipsum() + theme(plot.title = element_text(size=15)) grid.arrange(ggp1, ggp2, ncol = 2) 2.3.3 Imputación de la media Algunos científicos de datos o estadísticos pueden buscar una solución rápida sustituyendo los datos que faltan por la media. La media se utiliza a menudo para imputar datos categóricos. Por ejemplo, en el conjunto de datos de calidad del aire, supongamos que queremos imputar la media de sus valores perdidos. En este caso, utilizamos el paquete R mice. Cambiando el argumento method = mean, se especifica la imputación de la media, el argumento m = 1 cambia las iteraciones a 1, lo que significa no (iteración). El fundamento teórico de utilizar la media para imputar los datos perdidos es que la media es una buena estimación para seleccionar aleatoriamente una observación de una distribución normal. Ahora, intentaremos imputar la media para las variables Ozono y Solar.R del conjunto de datos airquality. En primer lugar, vamos a cargar los paquetes mice y mipackages (instale el paquete de CRAN primero usando install.packages(\"mice\") y install.packages(\"mi\")). Para mas información sobre el paquete utlizado visitar https://cran.r-project.org/web/packages/mice/mice.pdf. Podemos recuperar el conjunto de datos completo utilizando la función complete(). library(mice) library(foreign) La media de las variables Ozono y Solar.R puede ser imputada por la función mice(). imp &lt;- mice(data, m=5, maxit=50, method =&#39;pmm&#39;, seed=500, printFlag = FALSE) Donde m=5 se refiere al número de conjuntos de datos imputados. Cinco es el valor por defecto. meth='pmm' se refiere al método de imputación. En este caso, utilizamos el método de imputación de coincidencia de medias predictivas. Se pueden utilizar otros métodos de imputación, escriba methods(mice) para obtener una lista de los métodos de imputación disponibles. imp_df &lt;- complete(imp) head(imp_df) ## Ozone Solar.R Wind Temp ## 1 41 190 7.4 87 ## 2 36 118 8.0 80 ## 3 12 149 12.6 74 ## 4 18 313 10.9 66 ## 5 13 81 16.6 57 ## 6 28 78 7.4 66 Veamos un histograma y un diagrama de dispersión del aspecto del conjunto de datos de la calidad del aire tras la imputación de la media ggp1 &lt;- ggplot(data.frame(value=data$Ozone), aes(x=value)) + geom_histogram(fill=&quot;#FBD000&quot;, color=&quot;#E52521&quot;, alpha=0.9) + ggtitle(&quot;Original data&quot;) + xlab(&#39;Ozone&#39;) + ylab(&#39;Frequency&#39;) + theme_ipsum() + theme(plot.title = element_text(size=15)) ggp2 &lt;- ggplot(data.frame(value=imp_df$Ozone), aes(x=value)) + geom_histogram(fill=&quot;#43B047&quot;, color=&quot;#049CD8&quot;, alpha=0.9) + ggtitle(&quot;Mean imputation&quot;) + xlab(&#39;Ozone&#39;) + ylab(&#39;Frequency&#39;) + theme_ipsum() + theme(plot.title = element_text(size=15)) grid.arrange(ggp1, ggp2, ncol = 2) Vamos a comparar las distribuciones de los datos originales e imputados utilizando algunos gráficos útiles. En primer lugar, podemos utilizar un gráfico de dispersión y comparar Ozone con todas las demás variables library(lattice) xyplot(imp, Ozone ~ Wind + Temp + Solar.R, pch=18, cex=1) Lo que queremos ver es que la forma de los puntos (imputados) coincida con la de los azules (observados). La coincidencia de la forma nos indica que los valores imputados son realmente valores plausibles. Otro gráfico útil es el de la densidad: densityplot(imp) La densidad de los datos imputados para cada conjunto de datos imputados se muestra en color magenta, mientras que la densidad de los datos observados se muestra en azul. De nuevo, según nuestros supuestos anteriores, esperamos que las distribuciones sean similares. Es muy recomendable comprobar visualmente la convergencia. Lo comprobamos cuando llamamos a la función de trazado en la variable a la que asignamos la salida de mice, para mostrar gráficos de seguimiento de la media y desviación estándar de todas las variables implicadas en las imputaciones. plot(imp) Cada línea es una de las m imputaciones. Como se puede ver en el gráfico de trazado anterior sobre imp, no hay tendencias claras y las variables se superponen de una iteración a la siguiente. Dicho de otro modo, la varianza dentro de una cadena (hay m cadenas) debería ser aproximadamente igual a la varianza entre las cadenas. Esto indica que se ha logrado la convergencia. Si no se logra la convergencia, se puede aumentar el número de iteraciones que mice emplea especificando explícitamente el parámetro maxit a la función mice. Los siguientes son ejemplos de no convergencia usando el alogritmo mice 2.3.4 Otras técnicas de imputación La función mice entrega distintos métodos de imputación los cuales puede estudiar y aplicar, de acuerdo a lo que requieran sus datos. Puede aplicar la imputación por regresión en R con la configuración del método method = \"norm.predict\" en la función mice. Puede aplicar la imputación de regresión estocástica en R con la función mice utilizando el método method = \"norm.nob\". El paquete mice también incluye un procedimiento de imputación de regresión estocástica bayesiana. Puede aplicar este procedimiento de imputación con la función mice y utilizar como método method = \"norm\". Para estos métodos debe seleccionar primero dos columna del conjunto de datos de interes para ajustar constantes del modelo. data &lt;- data[, c(&quot;Solar.R&quot;, &quot;Wind&quot;)] imp.regress &lt;- mice(data, method=&quot;norm.predict&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Solar.R Wind imp.regress$imp$Wind ## 1 ## 4 9.736111 ## 5 9.806622 ## 6 9.805925 ## 7 9.743832 ## 8 9.854140 ## 9 9.898263 ## 10 9.801744 En la actualidad, hay un número limitado de análisis que pueden ser automáticamente agrupados por mice el más importante es el de lm/glm. Sin embargo, si se recuerda, el modelo lineal generalizado es extremadamente flexible, y puede utilizarse para expresar una amplia gama de análisis diferentes. Por extensión, podríamos utilizar la imputación múltiple no sólo para regresión lineal, sino para la regresión logística, la regresión de Poisson, las pruebas \\(t\\), el ANOVA ANCOVA, etc. Cada una de estas temáticas pueden ser abordadas como proyectos en este curso. 2.4 Detección de valores atípicos Un valor atípico es un valor o una observación que se aleja de otras observaciones, es decir, un punto de datos que difiere significativamente de otros puntos de datos. Enderlein (1987) va incluso más allá, ya que el autor considera que los valores atípicos son aquellos que se desvían tanto de otras observaciones que se podría suponer un mecanismo de muestreo subyacente diferente. Una observación debe compararse siempre con otras observaciones realizadas sobre el mismo fenómeno antes de calificarla realmente de atípica. En efecto, una persona de 200 cm de altura (1,90 m en EE.UU.) se considerará probablemente un valor atípico en comparación con la población general, pero esa misma persona podría no considerarse un valor atípico si midiéramos la altura de los jugadores de baloncesto. En esta sección, presentamos varios enfoques para detectar valores atípicos en R, desde técnicas sencillas como la estadística descriptiva (que incluye el mínimo, el máximo, el histograma, el boxplot y los percentiles) hasta técnicas más formales como el filtro de Hampel, el Grubbs, el Dixon y las pruebas de Rosner para detectar valores atípicos. Algunas pruebas estadísticas exigen la ausencia de valores atípicos para sacar conclusiones sólidas, pero la eliminación de valores atípicos no se recomienda en todos los casos y debe hacerse con precaución. 2.4.1 Estadísticas descriptivas El primer paso para detectar los valores atípicos en R es comenzar con algunas estadísticas descriptivas, y en particular con el mínimo y el máximo. En R, esto se puede hacer fácilmente con la función summary() dat &lt;- ggplot2::mpg summary(dat$hwy) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12.00 18.00 24.00 23.44 27.00 44.00 Nótese que el mínimo y el máximo son, respectivamente, el primer y el último valor de la salida anterior. Como alternativa, también pueden calcularse con las funciones min() y max() min(dat$hwy) ## [1] 12 max(dat$hwy) ## [1] 44 Un claro error de codificación, como un peso de 786 kg (1733 libras) para un humano, ya se detectará fácilmente con esta técnica tan sencilla. 2.4.2 Histogramas Otra forma básica de detectar valores atípicos es dibujar un histograma de los datos. Utilizando la base de R (con el número de bins correspondiente a la raíz cuadrada del número de observaciones para tener más bins que la opción por defecto), o usando ggplot2 hist(dat$hwy, xlab = &quot;hwy&quot;, main = &quot;Histogram of hwy&quot;, breaks = sqrt(nrow(dat)) ) library(ggplot2) ggplot(dat) + aes(x = hwy) + geom_histogram(bins = 30L, fill = &quot;#0c4c8a&quot;) + theme_minimal() Según el histograma, parece que hay un par de observaciones más altas que todas las demás 2.4.3 Boxplot Además de los histogramas, los boxplots también son útiles para detectar posibles valores atípicos boxplot(dat$hwy, ylab = &quot;hwy&quot; ) ggplot(dat) + aes(x = &quot;&quot;, y = hwy) + geom_boxplot(fill = &quot;#0c4c8a&quot;) + theme_minimal() Un diagrama de caja ayuda a visualizar una variable cuantitativa mostrando cinco resúmenes de localización comunes (mínimo, mediana, primer y tercer cuartil y máximo) y cualquier observación que se haya clasificado como presunto valor atípico utilizando el criterio del rango intercuartílico (IQR). Las observaciones consideradas como posibles valores atípicos según el criterio IQR se muestran como puntos en el diagrama de caja. Según este criterio, hay 2 valores atípicos potenciales (véanse los 2 puntos por encima de la línea vertical, en la parte superior del boxplot). Recuerde que no por el hecho de que una observación sea considerada como un valor atípico potencial por el criterio IQR debe eliminarla. Eliminar o mantener un valor atípico depende de (i) el contexto de su análisis, (ii) si las pruebas que va a realizar en el conjunto de datos son robustas a los valores atípicos o no, y (iii) a qué distancia está el valor atípico de otras observaciones. También es posible extraer los valores de los posibles valores atípicos basándose en el criterio IQR gracias a la función boxplot.stats()$out: boxplot.stats(dat$hwy)$out ## [1] 44 44 41 Como puede ver, en realidad hay 3 puntos considerados como posibles valores atípicos: 2 observaciones con un valor de 44 y 1 observación con un valor de 41. Gracias a la función which() es posible extraer el número de fila correspondiente a estos valores atípicos: out &lt;- boxplot.stats(dat$hwy)$out out_ind &lt;- which(dat$hwy %in% c(out)) out_ind ## [1] 213 222 223 Con esta información, ahora puede volver fácilmente a las filas específicas del conjunto de datos para verificarlas, o imprimir todas las variables para estos valores atípicos: dat[out_ind, ] ## # A tibble: 3 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 volkswagen jetta 1.9 1999 4 manua~ f 33 44 d comp~ ## 2 volkswagen new beetle 1.9 1999 4 manua~ f 35 44 d subc~ ## 3 volkswagen new beetle 1.9 1999 4 auto(~ f 29 41 d subc~ También es posible imprimir los valores de los valores atípicos directamente en el boxplot con la función mtext() boxplot(dat$hwy, ylab = &quot;hwy&quot;, main = &quot;Boxplot of highway miles per gallon&quot; ) mtext(paste(&quot;Outliers: &quot;, paste(out, collapse = &quot;, &quot;))) 2.4.4 Percentiles Este método de detección de valores atípicos se basa en los percentiles. Con el método de los percentiles, todas las observaciones que se encuentren fuera del intervalo formado por los percentiles 2.5 y 97.5 se considerarán como posibles valores atípicos. También pueden considerarse otros percentiles, como el 1 y el 99, o el 5 y el 95, para construir el intervalo. \\[ I=[q_{0.25}-1.5\\cdot\\textsf{IQR}; q_{0.75}+1.5\\cdot\\textsf{IQR}] \\] Los valores de los percentiles inferior y superior (y, por tanto, los límites inferior y superior del intervalo) pueden calcularse con la función quantile() lower_bound &lt;- quantile(dat$hwy, 0.025) lower_bound ## 2.5% ## 14 upper_bound &lt;- quantile(dat$hwy, 0.975) upper_bound ## 97.5% ## 35.175 Según este método, todas las observaciones por debajo de 14 y por encima de 35,175 se considerarán posibles valores atípicos. Los números de fila de las observaciones fuera del intervalo pueden extraerse entonces con la función which() outlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound) outlier_ind ## [1] 55 60 66 70 106 107 127 197 213 222 223 dat[outlier_ind, &quot;hwy&quot;] ## # A tibble: 11 x 1 ## hwy ## &lt;int&gt; ## 1 12 ## 2 12 ## 3 12 ## 4 12 ## 5 36 ## 6 36 ## 7 12 ## 8 37 ## 9 44 ## 10 44 ## 11 41 dat[outlier_ind, ] ## # A tibble: 11 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 dodge dakota ~ 4.7 2008 8 auto(~ 4 9 12 e pickup ## 2 dodge durango~ 4.7 2008 8 auto(~ 4 9 12 e suv ## 3 dodge ram 150~ 4.7 2008 8 auto(~ 4 9 12 e pickup ## 4 dodge ram 150~ 4.7 2008 8 manua~ 4 9 12 e pickup ## 5 honda civic 1.8 2008 4 auto(~ f 25 36 r subco~ ## 6 honda civic 1.8 2008 4 auto(~ f 24 36 c subco~ ## 7 jeep grand c~ 4.7 2008 8 auto(~ 4 9 12 e suv ## 8 toyota corolla 1.8 2008 4 manua~ f 28 37 r compa~ ## 9 volkswagen jetta 1.9 1999 4 manua~ f 33 44 d compa~ ## 10 volkswagen new bee~ 1.9 1999 4 manua~ f 35 44 d subco~ ## 11 volkswagen new bee~ 1.9 1999 4 auto(~ f 29 41 d subco~ Hay 11 valores atípicos potenciales según el método de los percentiles. Para reducir este número, puede establecer los percentiles en 1 y 99: lower_bound &lt;- quantile(dat$hwy, 0.01) upper_bound &lt;- quantile(dat$hwy, 0.99) outlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound) dat[outlier_ind, ] ## # A tibble: 3 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 volkswagen jetta 1.9 1999 4 manua~ f 33 44 d comp~ ## 2 volkswagen new beetle 1.9 1999 4 manua~ f 35 44 d subc~ ## 3 volkswagen new beetle 1.9 1999 4 auto(~ f 29 41 d subc~ 2.4.5 Filtro de Hampel Otro método, conocido como filtro de Hampel, consiste en considerar como valores atípicos los valores fuera del intervalo formado por la mediana, más o menos 3 desviaciones absolutas de la mediana \\[ I=[\\tilde{X}-3\\cdot\\textsf{MAD}, \\tilde{X}+3\\cdot\\textsf{MAD}], \\] donde MAD es la desviación absoluta de la mediana y se define como la mediana de las desviaciones absolutas de la mediana \\(\\tilde{X}=\\textsf{median}(X)\\) de los datos \\[ \\textsf{MAD}=\\textsf{median}(|X_{i}-\\tilde{X}|) \\] - Para este método, primero establecemos los límites del intervalo, gracias a las funciones median() y mad() lower_bound &lt;- median(dat$hwy) - 3 * mad(dat$hwy, constant = 1) lower_bound ## [1] 9 upper_bound &lt;- median(dat$hwy) + 3 * mad(dat$hwy, constant = 1) upper_bound ## [1] 39 Según este método, todas las observaciones por debajo de 9 y por encima de 39 se considerarán como posibles valores atípicos. Los números de fila de las observaciones que están fuera del intervalo pueden entonces extraerse con la función which(). Según el filtro de Hampel, hay 3 valores atípicos para la variable hwy. outlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound) outlier_ind ## [1] 213 222 223 2.4.6 Prueba de Grubbs La prueba de Grubbs permite detectar si el valor más alto o más bajo de un conjunto de datos es un valor atípico. La prueba de Grubbs detecta un valor atípico cada vez (valor más alto o más bajo), por lo que las hipótesis nula y alternativa son las siguientes \\(H_{0}\\): El valor más alto/bajo no es un valor atípico \\(H_{1}\\): El valor más alto/bajo es un valor atípico Como para cualquier prueba estadística, si el valor \\(p\\) es inferior al umbral de significación elegido (generalmente \\(\\alpha=0.05\\)), se rechaza la hipótesis nula y se concluye que el valor más bajo/más alto es un valor atípico. Por el contrario, si el valor \\(p\\) es mayor o igual que el nivel de significación, no se rechaza la hipótesis nula y concluiremos que, basándonos en los datos, no rechazamos la hipótesis de que el valor más bajo/más alto no es un valor atípico. Tenga en cuenta que la prueba de Grubbs no es apropiada para un tamaño de muestra \\(n\\leq6\\). Para realizar la prueba de Grubbs en R, utilizamos la función grubbs.test() del paquete outliers library(outliers) test &lt;- grubbs.test(dat$hwy) test ## ## Grubbs test for one outlier ## ## data: dat$hwy ## G = 3.45274, U = 0.94862, p-value = 0.05555 ## alternative hypothesis: highest value 44 is an outlier El valor \\(p\\) es de 0.056. Al nivel de significación del 5%, no rechazamos la hipótesis de que el valor más alto 44 no es un valor atípico. Por defecto, la prueba se realiza sobre el valor más alto (como se muestra en la salida de R: hipótesis alternativa: el valor más alto 44 es un valor atípico). Si desea realizar la prueba para el valor más bajo, simplemente añada el argumento opposite = TRUE en la función grubbs.test() test &lt;- grubbs.test(dat$hwy, opposite = TRUE) test ## ## Grubbs test for one outlier ## ## data: dat$hwy ## G = 1.92122, U = 0.98409, p-value = 1 ## alternative hypothesis: lowest value 12 is an outlier La salida de R indica que la prueba se realiza ahora sobre el valor más bajo (véase la hipótesis alternativa: el valor más bajo 12 es un valor atípico). El valor \\(p\\) es 1. Al nivel de significación del 5%, no rechazamos la hipótesis de que el valor más bajo 12 no es un valor atípico. A modo de ilustración, sustituiremos ahora una observación por un valor más extremo y realizaremos la prueba de Grubbs en este nuevo conjunto de datos. Reemplacemos el \\(34^{\\text{th}}\\) por un valor de 212 dat[34, &quot;hwy&quot;] &lt;- 212 Y ahora aplicamos la prueba de Grubbs para comprobar si el valor más alto es un valor atípico test &lt;- grubbs.test(dat$hwy) test ## ## Grubbs test for one outlier ## ## data: dat$hwy ## G = 13.72240, U = 0.18836, p-value &lt; 2.2e-16 ## alternative hypothesis: highest value 212 is an outlier El valor \\(p\\) es &lt; 0.001. Al nivel de significación del 5%, concluimos que el valor más alto 212 es un valor atípico. 2.4.7 Prueba de Dixon Al igual que la prueba de Grubbs, la prueba de Dixon se utiliza para comprobar si un único valor bajo o alto es un valor atípico. Por lo tanto, si se sospecha que hay más de un valor atípico, la prueba tiene que realizarse en estos valores atípicos sospechosos individualmente. Tenga en cuenta que la prueba de Dixon es más útil para muestras de pequeño tamaño (normalmente $n). Para realizar la prueba de Dixon en R, utilizamos la función dixon.test() del paquete outliers. Sin embargo, restringimos nuestro conjunto de datos a las 20 primeras observaciones, ya que la prueba de Dixon sólo se puede realizar en muestras de pequeño tamaño (R arrojará un error y sólo acepta conjuntos de datos de 3 a 30 observaciones) subdat &lt;- dat[1:20, ] test &lt;- dixon.test(subdat$hwy) test ## ## Dixon test for outliers ## ## data: subdat$hwy ## Q = 0.57143, p-value = 0.006508 ## alternative hypothesis: lowest value 15 is an outlier Los resultados muestran que el valor más bajo, 15, es un valor atípico (valor \\(p = 0.007\\)). Para comprobar el valor más alto, basta con añadir el argumento opuesto = TRUE a la función dixon.test() test &lt;- dixon.test(subdat$hwy, opposite = TRUE) test ## ## Dixon test for outliers ## ## data: subdat$hwy ## Q = 0.25, p-value = 0.8582 ## alternative hypothesis: highest value 31 is an outlier Los resultados muestran que el valor más alto 31 no es un valor atípico (valor \\(p = 0.858\\)). Es una buena práctica comprobar siempre los resultados de la prueba estadística de valores atípicos con el diagrama de caja para asegurarse de que hemos comprobado todos los valores atípicos potenciales out &lt;- boxplot.stats(subdat$hwy)$out boxplot(subdat$hwy, ylab = &quot;hwy&quot;) mtext(paste(&quot;Outliers: &quot;, paste(out, collapse = &quot;, &quot;))) A partir del boxplot, vemos que también podríamos aplicar la prueba de Dixon sobre el valor 20 además del valor 15 realizado anteriormente. Esto puede hacerse encontrando el número de fila del valor mínimo, excluyendo este número de fila del conjunto de datos y aplicando finalmente la prueba de Dixon a este nuevo conjunto de datos remove_ind &lt;- which.min(subdat$hwy) subsubdat &lt;- subdat[-remove_ind, ] test &lt;- dixon.test(subsubdat$hwy) test ## ## Dixon test for outliers ## ## data: subsubdat$hwy ## Q = 0.44444, p-value = 0.1297 ## alternative hypothesis: lowest value 20 is an outlier Los resultados muestran que el segundo valor más bajo, 20, no es un valor atípico (valor \\(p\\) = 0.13). 2.4.8 Prueba de Rosner La prueba de Rosner para detectar valores atípicos tiene las siguientes ventajas. Se utiliza para detectar varios valores atípicos a la vez (a diferencia de la prueba de Grubbs y Dixon, que debe realizarse de forma iterativa para detectar múltiples valores atípicos), y está diseñado para evitar el problema del enmascaramiento, en el que un valor atípico cercano a otro atípico puede pasar desapercibido. A diferencia de la prueba de Dixon, hay que tener en cuenta que la prueba de Rosner es más apropiada cuando el tamaño de la muestra es grande (\\(n\\geq 20\\)). Por tanto, volvemos a utilizar el conjunto de datos inicial dat, que incluye 234 observaciones. Para realizar la prueba de Rosner utilizamos la función rosnerTest() del paquete EnvStats. Esta función requiere al menos 2 argumentos: los datos y el número de presuntos valores atípicos k (con k = 3 como número de presuntos valores atípicos por defecto). Para este ejemplo, establecemos que el número de presuntos valores atípicos sea igual a 3, tal y como sugiere el número de posibles valores atípicos esbozado en el boxplot al principio del artículo. library(EnvStats) test &lt;- rosnerTest(dat$hwy, k = 3) test ## $distribution ## [1] &quot;Normal&quot; ## ## $statistic ## R.1 R.2 R.3 ## 13.722399 3.459098 3.559936 ## ## $sample.size ## [1] 234 ## ## $parameters ## k ## 3 ## ## $alpha ## [1] 0.05 ## ## $crit.value ## lambda.1 lambda.2 lambda.3 ## 3.652091 3.650836 3.649575 ## ## $n.outliers ## [1] 1 ## ## $alternative ## [1] &quot;Up to 3 observations are not\\n from the same Distribution.&quot; ## ## $method ## [1] &quot;Rosner&#39;s Test for Outliers&quot; ## ## $data ## [1] 29 29 31 30 26 26 27 26 25 28 27 25 25 25 25 24 25 23 ## [19] 20 15 20 17 17 26 23 26 25 24 19 14 15 17 27 212 26 29 ## [37] 26 24 24 22 22 24 24 17 22 21 23 23 19 18 17 17 19 19 ## [55] 12 17 15 17 17 12 17 16 18 15 16 12 17 17 16 12 15 16 ## [73] 17 15 17 17 18 17 19 17 19 19 17 17 17 16 16 17 15 17 ## [91] 26 25 26 24 21 22 23 22 20 33 32 32 29 32 34 36 36 29 ## [109] 26 27 30 31 26 26 28 26 29 28 27 24 24 24 22 19 20 17 ## [127] 12 19 18 14 15 18 18 15 17 16 18 17 19 19 17 29 27 31 ## [145] 32 27 26 26 25 25 17 17 20 18 26 26 27 28 25 25 24 27 ## [163] 25 26 23 26 26 26 26 25 27 25 27 20 20 19 17 20 17 29 ## [181] 27 31 31 26 26 28 27 29 31 31 26 26 27 30 33 35 37 35 ## [199] 15 18 20 20 22 17 19 18 20 29 26 29 29 24 44 29 26 29 ## [217] 29 29 29 23 24 44 41 29 26 28 29 29 29 28 29 26 26 26 ## ## $data.name ## [1] &quot;dat$hwy&quot; ## ## $bad.obs ## [1] 0 ## ## $all.stats ## i Mean.i SD.i Value Obs.Num R.i+1 lambda.i+1 Outlier ## 1 0 24.21795 13.684345 212 34 13.722399 3.652091 TRUE ## 2 1 23.41202 5.951835 44 213 3.459098 3.650836 FALSE ## 3 2 23.32328 5.808172 44 222 3.559936 3.649575 FALSE ## ## attr(,&quot;class&quot;) ## [1] &quot;gofOutlier&quot; Los resultados interesantes se ofrecen en la tabla all.stats test$all.stats ## i Mean.i SD.i Value Obs.Num R.i+1 lambda.i+1 Outlier ## 1 0 24.21795 13.684345 212 34 13.722399 3.652091 TRUE ## 2 1 23.41202 5.951835 44 213 3.459098 3.650836 FALSE ## 3 2 23.32328 5.808172 44 222 3.559936 3.649575 FALSE Basándonos en la prueba de Rosner, vemos que sólo hay un valor atípico (véase la columna de valores atípicos), y que es la observación 34 (véase Obs.Num) con un valor de 212 (véase Value). 2.4.9 Tratamiento de los valores atípicos Una vez identificados los valores atípicos y habiendo decidido enmendar la situación según la naturaleza del problema, puede considerar uno de los siguientes enfoques. Imputación Este método se ha tratado en detalle en la discusión sobre el tratamiento de los valores faltantes. Capping Para los valores que se encuentran fuera de los límites de \\(15\\cdot\\mathsf{IQR}\\), podríamos poner un tope sustituyendo las observaciones que se encuentran fuera del límite inferior por el valor del \\(5^{\\textsf{th}}\\) percentile y las que se encuentran por encima del límite superior, por el valor del \\(95^{\\textsf{th}}\\) percentile. A continuación se muestra un código de ejemplo que logra esto x &lt;- dat$hwy qnt &lt;- quantile(x, probs=c(.25, .75), na.rm = T) caps &lt;- quantile(x, probs=c(.05, .95), na.rm = T) H &lt;- 1.5 * IQR(x, na.rm = T) x[x &lt; (qnt[1] - H)] &lt;- caps[1] x[x &gt; (qnt[2] + H)] &lt;- caps[2] head(x) ## [1] 29 29 31 30 26 26 Predicción En otro enfoque, los valores atípicos pueden sustituirse por valores perdidos (NA) y luego pueden predecirse considerándolos como una variable de respuesta. Ya hemos hablado de cómo predecir los valores perdidos en la sección anterior. "],["introducción-a-postgresql-en-r.html", "Chapter 3 Introducción a PostgreSQL en R 3.1 Conexión con R 3.2 Importación de tablas 3.3 Candlesticks plot", " Chapter 3 Introducción a PostgreSQL en R Cuando se trata de grandes conjuntos de datos que potencialmente exceden la memoria de su máquina, es bueno tener otra posibilidad, como su propio servidor con una base de datos SQL/PostgreSQL, donde se puede consultar los datos. Por ejemplo, un conjunto de datos financieros de 5 GB caben en una memoria RAM básica, pero los datos consumen muchos recursos. Una solución es utilizar una base de datos basada en SQL, donde puedo consultar los datos en trozos más pequeños, dejando recursos para el cálculo. Aunque MySQL es la más utilizada, PostgreSQL tiene la ventaja de ser de código abierto y gratuita para todos los usos. Sin embargo, todavía tenemos que conseguir un servidor. Una forma posible de hacerlo es alquilar un servidor de Amazon, sin embargo, existe la opción de utilizar Heroku Postgres tal como se explicó en la sección de Python. 3.1 Conexión con R Ahora es el momento de conectarse a la base de datos con R. Este enfoque utiliza el paquete RPostgreSQL. Los siguientes paquetes y herramientas deben ser instaladas para poder hacer uso de la API y realizar la conexión con éxito RPostgresql DBI Su propia base de datos PostgreSQL Acceso remoto a su base de datos El paquete RPostgreSQL y DBI se puede instalar desde CRAN o Github install.packages(&quot;RPostgreSQL&quot;) install.packages(&quot;DBI&quot;) devtools::install_git(&#39;https://github.com/r-dbi/DBI.git&#39;) devtools::install_git(&#39;https://github.com/cran/RPostgreSQL.git) Para conectar, necesitamos introducir los siguientes comandos en R, nos conectaremos a la base de datos creada anteriormente en Heroku Postgres en la sección de Python. RPostgres es una interfaz compatible con DBI para la base de datos Postgres. Es una reescritura desde cero usando C++ y Rcpp. Este paquete actúa tanto como el controlador de la base de datos como la interfaz DBI. El código y la información adicional están disponibles en su repositorio GitHub aquí: RPostgres library(DBI) library(RPostgreSQL) con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;d4oenn2g04nlqm&quot;, host = &quot;ec2-34-205-14-168.compute-1.amazonaws.com&quot;, port = 5432, user = &quot;xoihbshcjgwwtd&quot;, password = &quot;aac4c765bf11a3982c859f19dcd57848c55b6e7b713fb98ca05e3711f2b9d7e9&quot;) Para visualizar la lista de tablas que hemos importado antes en la base de datos utilizamos dbListTables() dbListTables(conn = con) ## [1] &quot;accounts&quot; &quot;company&quot; &quot;tecnoglass&quot; &quot;ethereum&quot; 3.2 Importación de tablas Carguemos como dataframe un archivo CSV que contiene los precios de las acciones de Tecnoglass Inc. (TGLS), empresa Barranquillera que cotiza en Nasdaq. Para esto usamos la función read.csv() TGLS_df &lt;- read.csv(file = &#39;TGLS.csv&#39;) head(TGLS_df) ## Date Open High Low Close Adj.Close Volume ## 1 2012-05-10 9.97 10.00 9.50 9.80 7.905694 6900 ## 2 2012-05-11 9.70 9.70 9.70 9.70 7.825023 300 ## 3 2012-05-14 9.80 9.80 9.80 9.80 7.905694 100 ## 4 2012-05-15 9.75 9.75 9.75 9.75 7.865361 300 ## 5 2012-05-16 9.75 9.75 9.75 9.75 7.865361 0 ## 6 2012-05-17 9.60 9.60 9.60 9.60 7.744353 800 Los nombres de las columnas de este dataframe son problemáticos para las bases de datos (y especialmente para PostgreSQL) por varias razones: los \".\" en los nombres pueden ser un problema, y PostgreSQL espera que los nombres de las columnas estén todos en minúsculas. Aquí hay una función para hacer que los nombres de las columnas sean seguros para las bases de datos. Luego obtenemos algunos estadísticos usando la función summary() dbSafeNames = function(names) { names = gsub(&#39;[^a-z0-9]+&#39;,&#39;_&#39;,tolower(names)) names = make.names(names, unique=TRUE, allow_=TRUE) names = gsub(&#39;.&#39;,&#39;_&#39;,names, fixed=TRUE) names } colnames(TGLS_df) = dbSafeNames(colnames(TGLS_df)) head(TGLS_df) ## date open high low close adj_close volume ## 1 2012-05-10 9.97 10.00 9.50 9.80 7.905694 6900 ## 2 2012-05-11 9.70 9.70 9.70 9.70 7.825023 300 ## 3 2012-05-14 9.80 9.80 9.80 9.80 7.905694 100 ## 4 2012-05-15 9.75 9.75 9.75 9.75 7.865361 300 ## 5 2012-05-16 9.75 9.75 9.75 9.75 7.865361 0 ## 6 2012-05-17 9.60 9.60 9.60 9.60 7.744353 800 summary(TGLS_df) ## date open high low ## Length:2380 Min. : 2.440 Min. : 2.54 Min. : 2.150 ## Class :character 1st Qu.: 8.115 1st Qu.: 8.24 1st Qu.: 7.980 ## Mode :character Median : 9.910 Median : 9.93 Median : 9.870 ## Mean :10.087 Mean :10.24 Mean : 9.916 ## 3rd Qu.:11.240 3rd Qu.:11.39 3rd Qu.:11.050 ## Max. :27.120 Max. :27.94 Max. :26.970 ## close adj_close volume ## Min. : 2.290 Min. : 2.232 Min. : 0 ## 1st Qu.: 8.088 1st Qu.: 7.592 1st Qu.: 2675 ## Median : 9.900 Median : 8.059 Median : 18150 ## Mean :10.076 Mean : 8.726 Mean : 55102 ## 3rd Qu.:11.220 3rd Qu.: 9.116 3rd Qu.: 47025 ## Max. :27.330 Max. :27.330 Max. :2249100 Ahora procedemos a insertar tabla TGLS_df en nuestra base de datos Heroku Postgres usando la función dbWriteTable() dbWriteTable(con, &#39;tecnoglass&#39;, TGLS_df, row.names=FALSE, overwrite=TRUE) La función dbWriteTable() devuelve TRUE si la tabla fue escrita con éxito. Tenga en cuenta que esta llamada fallará si ya existe la tabla en la base de datos. Utilice overwrite = TRUE para forzar la sobreescritura de una tabla existente, y append = TRUE para añadirla a una tabla existente. Verifique que la tabla fué creada usando pgAdmin Ahora puedes volver a leer la tabla usando dbGetQuery() o dbReadTable dtab = dbGetQuery(con, &quot;SELECT * FROM tecnoglass&quot;) summary(dtab) ## date open high low ## Length:2380 Min. : 2.440 Min. : 2.54 Min. : 2.150 ## Class :character 1st Qu.: 8.115 1st Qu.: 8.24 1st Qu.: 7.980 ## Mode :character Median : 9.910 Median : 9.93 Median : 9.870 ## Mean :10.087 Mean :10.24 Mean : 9.916 ## 3rd Qu.:11.240 3rd Qu.:11.39 3rd Qu.:11.050 ## Max. :27.120 Max. :27.94 Max. :26.970 ## close adj_close volume ## Min. : 2.290 Min. : 2.232 Min. : 0 ## 1st Qu.: 8.088 1st Qu.: 7.592 1st Qu.: 2675 ## Median : 9.900 Median : 8.059 Median : 18150 ## Mean :10.076 Mean : 8.726 Mean : 55102 ## 3rd Qu.:11.220 3rd Qu.: 9.116 3rd Qu.: 47025 ## Max. :27.330 Max. :27.330 Max. :2249100 rm(dtab) dtab = dbReadTable(con, &quot;tecnoglass&quot;) summary(dtab) ## date open high low ## Length:2380 Min. : 2.440 Min. : 2.54 Min. : 2.150 ## Class :character 1st Qu.: 8.115 1st Qu.: 8.24 1st Qu.: 7.980 ## Mode :character Median : 9.910 Median : 9.93 Median : 9.870 ## Mean :10.087 Mean :10.24 Mean : 9.916 ## 3rd Qu.:11.240 3rd Qu.:11.39 3rd Qu.:11.050 ## Max. :27.120 Max. :27.94 Max. :26.970 ## close adj_close volume ## Min. : 2.290 Min. : 2.232 Min. : 0 ## 1st Qu.: 8.088 1st Qu.: 7.592 1st Qu.: 2675 ## Median : 9.900 Median : 8.059 Median : 18150 ## Mean :10.076 Mean : 8.726 Mean : 55102 ## 3rd Qu.:11.220 3rd Qu.: 9.116 3rd Qu.: 47025 ## Max. :27.330 Max. :27.330 Max. :2249100 Por supuesto, el objetivo de utilizar una base de datos es extraer subconjuntos o transformaciones de sus datos, utilizando SQL. Para esto procedemos de la siguiente forma rm(dtab) dtab = dbGetQuery(con, &quot;SELECT date, open, close, high, low FROM tecnoglass&quot;) head(dtab) ## date open close high low ## 1 2012-05-10 9.97 9.80 10.00 9.50 ## 2 2012-05-11 9.70 9.70 9.70 9.70 ## 3 2012-05-14 9.80 9.80 9.80 9.80 ## 4 2012-05-15 9.75 9.75 9.75 9.75 ## 5 2012-05-16 9.75 9.75 9.75 9.75 ## 6 2012-05-17 9.60 9.60 9.60 9.60 Puedes utilizar dbSendQuery para enviar consultas que no devuelven un resultado tipo marco de datos. Luego asegúrese de enviar los cambios a la base de datos usando dbCommit dbBegin(con) dbSendQuery(con, &quot;DROP TABLE IF EXISTS tecnoglass&quot;) dbCommit(con) Cuando hayas terminado, desconecta dbDisconnect(con) 3.3 Candlesticks plot Usaremos los datos importados en la base de datos para realizar grafico de velas japonesas usando plotly. Inciamos realizando la conexión a nuestra base de datos, luego realizamos la figura utilizando la función plot_ly() usando type=\"candlestick\" library(DBI) library(RPostgreSQL) con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;d4oenn2g04nlqm&quot;, host = &quot;ec2-34-205-14-168.compute-1.amazonaws.com&quot;, port = 5432, user = &quot;xoihbshcjgwwtd&quot;, password = &quot;aac4c765bf11a3982c859f19dcd57848c55b6e7b713fb98ca05e3711f2b9d7e9&quot;) dtab = dbGetQuery(con, &quot;SELECT * FROM tecnoglass&quot;) head(dtab) ## date open high low close adj_close volume ## 1 2012-05-10 9.97 10.00 9.50 9.80 7.905694 6900 ## 2 2012-05-11 9.70 9.70 9.70 9.70 7.825023 300 ## 3 2012-05-14 9.80 9.80 9.80 9.80 7.905694 100 ## 4 2012-05-15 9.75 9.75 9.75 9.75 7.865361 300 ## 5 2012-05-16 9.75 9.75 9.75 9.75 7.865361 0 ## 6 2012-05-17 9.60 9.60 9.60 9.60 7.744353 800 library(plotly) fig &lt;- dtab %&gt;% plot_ly(x = ~date, type=&quot;candlestick&quot;, open = ~open, close = ~close, high = ~high, low = ~low) fig &lt;- fig %&gt;% layout(title = &quot;Basic Candlestick Chart&quot;, xaxis = list(title = &#39;Day&#39;), yaxis = list(title = &#39;TGLS-USD&#39;)) fig i &lt;- list(line = list(color = &#39;#FFD700&#39;)) d &lt;- list(line = list(color = &#39;#0000ff&#39;)) fig &lt;- dtab %&gt;% plot_ly(x = ~date, type=&quot;ohlc&quot;, open = ~open, close = ~close, high = ~high, low = ~low, increasing = i, decreasing = d) fig &lt;- fig %&gt;% layout(title = &quot;Basic OHLC Chart&quot;, xaxis = list(rangeslider = list(visible = F), title = &#39;Day&#39;), yaxis = list(title = &#39;TGLS-USD&#39;)) fig dbDisconnect(con) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
